{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify Brain MRI Images (Multi-Class Classification)\n",
    "\n",
    "## Pre-requisites\n",
    "Install [kagglehub](https://pypi.org/project/kagglehub/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load Dataset\n",
    "### 1.1 Download Data and Generate Annotation Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pbd0/miniforge3/envs/3321/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Download dataset and locate it in machine\n",
    "data_dirname = kagglehub.dataset_download(\"masoudnickparvar/brain-tumor-mri-dataset\")\n",
    "# print(data_dirname)\n",
    "train_dirname = os.path.join(data_dirname, 'Training')\n",
    "test_dirname = os.path.join(data_dirname, 'Testing')\n",
    "classes = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
    "\n",
    "# Get training files\n",
    "tr_gl_files = glob(os.path.join(train_dirname, classes[0], '*.jpg'))\n",
    "tr_me_files = glob(os.path.join(train_dirname, classes[1], '*.jpg'))\n",
    "tr_no_files = glob(os.path.join(train_dirname, classes[2], '*.jpg'))\n",
    "tr_pi_files = glob(os.path.join(train_dirname, classes[3], '*.jpg'))\n",
    "# print(len(tr_gl_files), len(tr_me_files), len(tr_no_files), len(tr_pi_files))\n",
    "train_files = tr_gl_files + tr_me_files + tr_no_files + tr_pi_files\n",
    "train_labels = [classes[0]] * len(tr_gl_files) + \\\n",
    "    [classes[1]] * len(tr_me_files) + \\\n",
    "    [classes[2]] * len(tr_no_files) + \\\n",
    "    [classes[-1]] * len(tr_pi_files)\n",
    "train_dict = {'path': train_files, 'label': train_labels}\n",
    "df_train = pd.DataFrame(train_dict)\n",
    "# print(df_train)\n",
    "df_train.to_csv('annotation_train.csv', header=False, index=False)\n",
    "\n",
    "# Get testing files\n",
    "te_gl_files = glob(os.path.join(test_dirname, classes[0], '*.jpg'))\n",
    "te_me_files = glob(os.path.join(test_dirname, classes[1], '*.jpg'))\n",
    "te_no_files = glob(os.path.join(test_dirname, classes[2], '*.jpg'))\n",
    "te_pi_files = glob(os.path.join(test_dirname, classes[3], '*.jpg'))\n",
    "# print(len(te_gl_files), len(te_me_files), len(te_no_files), len(te_pi_files))\n",
    "test_files = te_gl_files + te_me_files + te_no_files + te_pi_files\n",
    "test_labels = [classes[0]] * len(te_gl_files) + \\\n",
    "    [classes[1]] * len(te_me_files) + \\\n",
    "    [classes[2]] * len(te_no_files) + \\\n",
    "    [classes[-1]] * len(te_pi_files)\n",
    "test_dict = {'path': test_files, 'label': test_labels}\n",
    "df_test = pd.DataFrame(test_dict)\n",
    "# print(df_train)\n",
    "df_test.to_csv('annotation_test.csv', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "\n",
    "class TumorDataset(Dataset):\n",
    "    def __init__(self, annotations_file):\n",
    "        self.imgs_info = pd.read_csv(annotations_file, header=None)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_info)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_path = self.imgs_info.iloc[idx, 0]\n",
    "        img_raw = cv.imread(img_path, cv.IMREAD_GRAYSCALE)\n",
    "        image = cv.resize(img_raw, (128, 128))\n",
    "        if self.imgs_info.iloc[idx, 1] == classes[0]:\n",
    "            category = 0\n",
    "        elif self.imgs_info.iloc[idx, 1] == classes[1]:\n",
    "            category = 1\n",
    "        elif self.imgs_info.iloc[idx, 1] == classes[2]:\n",
    "            category = 2\n",
    "        else:\n",
    "            category = 3\n",
    "        sample = {'image': image, 'category': category}\n",
    "        return sample\n",
    "    \n",
    "dataset_train = TumorDataset(annotations_file='annotation_train.csv')\n",
    "# for i, sample in enumerate(dataset_train):\n",
    "#     image = sample['image']\n",
    "#     label = sample['category']\n",
    "#     if not i%100:  # i % 100 != 0\n",
    "#         print(i, image.shape, label)\n",
    "# print(i, image.shape, label)\n",
    "dataset_test = TumorDataset(annotations_file='annotation_test.csv')\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=10000, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=10000, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Pre-Process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5712, 128, 128) (5712,)\n",
      "(1311, 128, 128) (1311,)\n",
      "(5712, 16384) (5712, 4)\n",
      "(1311, 16384) (1311, 4)\n",
      "[3 3 1 1]\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Extract features and labels from the dataloaders\n",
    "data_train = next(iter(dataloader_train))\n",
    "data_test = next(iter(dataloader_test))\n",
    "\n",
    "# Separate features and labels\n",
    "raw_features_train = data_train['image'].numpy()\n",
    "raw_features_test = data_test['image'].numpy()\n",
    "raw_labels_train = data_train['category'].numpy()\n",
    "raw_labels_test = data_test['category'].numpy()\n",
    "print(raw_features_train.shape, raw_labels_train.shape)\n",
    "print(raw_features_test.shape, raw_labels_test.shape)\n",
    "\n",
    "# Reshape\n",
    "reshaped_features_train = raw_features_train.reshape(raw_features_train.shape[0], -1)\n",
    "reshaped_features_test = raw_features_test.reshape(raw_features_test.shape[0], -1)\n",
    "onehot_labels_train = np.zeros((raw_labels_train.shape[0], len(classes)))\n",
    "onehot_labels_test = np.zeros((raw_labels_test.shape[0], len(classes)))\n",
    "print(reshaped_features_train.shape, onehot_labels_train.shape)\n",
    "print(reshaped_features_test.shape, onehot_labels_test.shape)\n",
    "\n",
    "# One-hot encoding\n",
    "onehot_labels_train[np.arange(raw_labels_train.shape[0]), raw_labels_train] = 1\n",
    "onehot_labels_test[np.arange(raw_labels_test.shape[0]), raw_labels_test] = 1\n",
    "print(raw_labels_train[:4])\n",
    "print(onehot_labels_train[:4])\n",
    "\n",
    "# Rescale\n",
    "rescaled_features_train = reshaped_features_train / 255\n",
    "rescaled_features_test = reshaped_features_test / 255\n",
    "# print(rescaled_features_test[0, 8000:8500])\n",
    "\n",
    "features_train = rescaled_features_train\n",
    "features_test = rescaled_features_test\n",
    "labels_train = onehot_labels_train\n",
    "labels_test = onehot_labels_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Build Multi-Layer Perceptron Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "[[0.1110943  0.11109574 0.11111844 0.1111291  0.11111472 0.11112763\n",
      "  0.11111259 0.11109647 0.11111101]\n",
      " [0.1110943  0.11109574 0.11111844 0.1111291  0.11111472 0.11112763\n",
      "  0.11111259 0.11109647 0.11111101]\n",
      " [0.1110943  0.11109574 0.11111844 0.1111291  0.11111472 0.11112763\n",
      "  0.11111259 0.11109647 0.11111101]\n",
      " [0.1110943  0.11109574 0.11111844 0.1111291  0.11111472 0.11112763\n",
      "  0.11111259 0.11109647 0.11111101]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def init_params(layer_sizes):\n",
    "    \"\"\"\n",
    "    layer_sizes: list/tuple\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    for i in range(len(layer_sizes)-1):\n",
    "        parameters['W' + str(i+1)] = np.random.normal(0, 0.0001, size=(layer_sizes[i+1], layer_sizes[i]))\n",
    "        parameters['b' + str(i+1)] = np.random.normal(0, 0.0001, size=(1, layer_sizes[i+1]))\n",
    "    return parameters\n",
    "\n",
    "# Sanity check\n",
    "dummy_layer_sizes = list(range(1, 10))\n",
    "dummy_params = init_params(dummy_layer_sizes)\n",
    "print(len(dummy_params))\n",
    "\n",
    "def linear(in_features, weight, bias):\n",
    "    return in_features @ weight.T + bias\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(out_features):\n",
    "    probs = np.exp(out_features) / np.sum(np.exp(out_features), axis=1, keepdims=True)\n",
    "    return probs\n",
    "\n",
    "def forward(features_input, params):\n",
    "    num_layers = int(len(params) / 2 - 1)\n",
    "    cache = {'X0': features_input}\n",
    "    for i in range(num_layers):\n",
    "        cache['Z' + str(i+1)] = linear(cache['X' + str(i)], params['W' + str(i+1)], params['b' + str(i+1)])\n",
    "        cache['X' + str(i+1)] = relu(cache['Z' + str(i+1)])\n",
    "    cache['Z' + str(i+2)] = linear(cache['X' + str(i+1)], params['W' + str(i+2)], params['b' + str(i+2)])\n",
    "    predictions = softmax(cache['Z' + str(i+2)])\n",
    "\n",
    "    return predictions, cache\n",
    "\n",
    "# Sanity check\n",
    "dummy_features = np.random.normal(size=(4, 1))\n",
    "dummy_preds, dummy_cache = forward(dummy_features, dummy_params)\n",
    "print(dummy_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "2.1972518681887108\n"
     ]
    }
   ],
   "source": [
    "def ce_loss(predictions, labels):\n",
    "    error = -np.sum(labels * np.log(predictions), axis=1)\n",
    "    return np.mean(error)\n",
    "\n",
    "# Sanity check\n",
    "dummy_labels = np.zeros((4, 9))\n",
    "dummy_labels[np.arange(4), np.random.randint(0, 9, (4,))] = 1\n",
    "dummy_loss = ce_loss(dummy_preds, dummy_labels)\n",
    "print(dummy_labels)\n",
    "print(dummy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Back-Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dZ8': array([[ 0.1110943 ,  0.11109574,  0.11111844,  0.1111291 ,  0.11111472,\n",
      "         0.11112763,  0.11111259,  0.11109647, -0.88888899],\n",
      "       [ 0.1110943 ,  0.11109574,  0.11111844, -0.8888709 ,  0.11111472,\n",
      "         0.11112763,  0.11111259,  0.11109647,  0.11111101],\n",
      "       [ 0.1110943 , -0.88890426,  0.11111844,  0.1111291 ,  0.11111472,\n",
      "         0.11112763,  0.11111259,  0.11109647,  0.11111101],\n",
      "       [ 0.1110943 ,  0.11109574,  0.11111844,  0.1111291 ,  0.11111472,\n",
      "         0.11112763,  0.11111259, -0.88890353,  0.11111101]]), 'dW8': array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         1.43102797e-05,  0.00000000e+00,  1.02453568e-05,\n",
      "         1.10178568e-04,  7.90144456e-05],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        -1.78925375e-05,  0.00000000e+00, -1.28100522e-05,\n",
      "        -1.37759303e-04, -9.87939407e-05],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         1.43133899e-05,  0.00000000e+00,  1.02475835e-05,\n",
      "         1.10202514e-04,  7.90316182e-05],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        -1.78882400e-05,  0.00000000e+00, -1.28069755e-05,\n",
      "        -1.37726216e-04, -9.87702123e-05],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         1.43129107e-05,  0.00000000e+00,  1.02472404e-05,\n",
      "         1.10198825e-04,  7.90289727e-05],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         1.43145735e-05,  0.00000000e+00,  1.02484309e-05,\n",
      "         1.10211628e-04,  7.90381539e-05],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         1.43126365e-05,  0.00000000e+00,  1.02470441e-05,\n",
      "         1.10196714e-04,  7.90274583e-05],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        -1.78924429e-05,  0.00000000e+00, -1.28099845e-05,\n",
      "        -1.37758575e-04, -9.87934186e-05],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        -1.78905699e-05,  0.00000000e+00, -1.28086436e-05,\n",
      "        -1.37744155e-04, -9.87830770e-05]]), 'db8': array([[ 0.1110943 , -0.13890426,  0.11111844, -0.1388709 ,  0.11111472,\n",
      "         0.11112763,  0.11111259, -0.13890353, -0.13888899]]), 'dX7': array([[ 1.26757423e-04, -9.05719127e-06,  7.61454168e-05,\n",
      "         5.41395502e-05, -5.89909536e-06,  6.28099305e-05,\n",
      "        -5.36089670e-06, -7.22315139e-05],\n",
      "       [ 7.54307258e-05,  1.16113401e-04, -4.37505264e-05,\n",
      "         1.00742284e-04, -2.64268587e-04, -3.10041752e-05,\n",
      "        -9.57750797e-05,  6.67033197e-05],\n",
      "       [-1.64645759e-05, -1.13207883e-04,  2.33320213e-05,\n",
      "        -2.14784342e-04, -1.85953030e-04,  4.42430487e-05,\n",
      "         1.13060738e-04, -4.71392772e-05],\n",
      "       [-1.61852313e-05, -6.10768648e-05, -1.88262552e-06,\n",
      "        -1.18007694e-04,  3.02666347e-04, -3.02993174e-05,\n",
      "         7.50629943e-06,  2.94103523e-05]]), 'dZ7': array([[ 0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
      "         5.41395502e-05, -0.00000000e+00,  6.28099305e-05,\n",
      "        -5.36089670e-06, -7.22315139e-05],\n",
      "       [ 0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
      "         1.00742284e-04, -0.00000000e+00, -3.10041752e-05,\n",
      "        -9.57750797e-05,  6.67033197e-05],\n",
      "       [-0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
      "        -2.14784342e-04, -0.00000000e+00,  4.42430487e-05,\n",
      "         1.13060738e-04, -4.71392772e-05],\n",
      "       [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
      "        -1.18007694e-04,  0.00000000e+00, -3.02993174e-05,\n",
      "         7.50629943e-06,  2.94103523e-05]]), 'dW7': array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [ 0.00000000e+00, -1.25277587e-08,  0.00000000e+00,\n",
      "         0.00000000e+00, -1.07646210e-08,  0.00000000e+00,\n",
      "        -2.84216874e-09],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [ 0.00000000e+00,  3.22150457e-09,  0.00000000e+00,\n",
      "         0.00000000e+00,  2.76811491e-09,  0.00000000e+00,\n",
      "         7.30861745e-10],\n",
      "       [ 0.00000000e+00,  1.36826130e-09,  0.00000000e+00,\n",
      "         0.00000000e+00,  1.17569429e-09,  0.00000000e+00,\n",
      "         3.10417019e-10],\n",
      "       [ 0.00000000e+00, -1.63767773e-09,  0.00000000e+00,\n",
      "         0.00000000e+00, -1.40719345e-09,  0.00000000e+00,\n",
      "        -3.71539440e-10]]), 'db7': array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        -4.44775505e-05,  0.00000000e+00,  1.14373717e-05,\n",
      "         4.85776529e-06, -5.81427977e-06]]), 'dX6': array([[ 2.49281295e-08,  8.61136052e-09,  8.25701793e-09,\n",
      "         1.61203820e-08, -3.12942466e-09,  4.23110642e-10,\n",
      "         7.78949995e-09],\n",
      "       [-3.05631620e-08,  7.36184813e-09,  8.97902560e-09,\n",
      "         4.90115410e-09,  2.83070761e-09,  1.19391198e-08,\n",
      "         1.30070949e-08],\n",
      "       [ 3.64007273e-08, -1.99931450e-08, -2.13217984e-08,\n",
      "        -1.03540940e-08,  7.10410753e-09, -1.90766814e-08,\n",
      "        -2.79918253e-08],\n",
      "       [-4.08836226e-09, -1.83123319e-08, -1.39020591e-08,\n",
      "        -1.34361993e-08,  9.23313963e-09, -3.90039052e-09,\n",
      "        -1.77327050e-08]]), 'dZ6': array([[ 0.00000000e+00,  8.61136052e-09,  0.00000000e+00,\n",
      "         0.00000000e+00, -3.12942466e-09,  0.00000000e+00,\n",
      "         7.78949995e-09],\n",
      "       [-0.00000000e+00,  7.36184813e-09,  0.00000000e+00,\n",
      "         0.00000000e+00,  2.83070761e-09,  0.00000000e+00,\n",
      "         1.30070949e-08],\n",
      "       [ 0.00000000e+00, -1.99931450e-08, -0.00000000e+00,\n",
      "        -0.00000000e+00,  7.10410753e-09, -0.00000000e+00,\n",
      "        -2.79918253e-08],\n",
      "       [-0.00000000e+00, -1.83123319e-08, -0.00000000e+00,\n",
      "        -0.00000000e+00,  9.23313963e-09, -0.00000000e+00,\n",
      "        -1.77327050e-08]]), 'dW6': array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00, -3.49058198e-12,\n",
      "         0.00000000e+00, -2.15690224e-12,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  2.50685706e-12,\n",
      "         0.00000000e+00,  1.54903842e-12,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00, -3.89629040e-12,\n",
      "         0.00000000e+00, -2.40759780e-12,  0.00000000e+00]]), 'db6': array([[ 0.00000000e+00, -5.58306705e-09,  0.00000000e+00,\n",
      "         0.00000000e+00,  4.00963253e-09,  0.00000000e+00,\n",
      "        -6.23198386e-09]]), 'dX5': array([[-1.66119790e-12,  7.14232026e-13,  7.44487673e-13,\n",
      "         1.83051652e-12,  1.46675376e-12, -3.18319912e-13],\n",
      "       [-2.05242454e-12,  1.81056201e-12,  7.11166897e-13,\n",
      "         3.68011429e-12,  1.87762097e-12, -1.44053886e-12],\n",
      "       [ 4.56666581e-12, -2.35353765e-12, -2.62816600e-12,\n",
      "        -6.23992042e-12, -4.88173711e-12,  1.50124013e-12],\n",
      "       [ 3.53097934e-12, -1.23810209e-12, -1.93048879e-12,\n",
      "        -3.72338590e-12, -3.44811583e-12,  4.39850667e-13]]), 'dZ5': array([[-0.00000000e+00,  0.00000000e+00,  7.44487673e-13,\n",
      "         0.00000000e+00,  1.46675376e-12, -0.00000000e+00],\n",
      "       [-0.00000000e+00,  0.00000000e+00,  7.11166897e-13,\n",
      "         0.00000000e+00,  1.87762097e-12, -0.00000000e+00],\n",
      "       [ 0.00000000e+00, -0.00000000e+00, -2.62816600e-12,\n",
      "        -0.00000000e+00, -4.88173711e-12,  0.00000000e+00],\n",
      "       [ 0.00000000e+00, -0.00000000e+00, -1.93048879e-12,\n",
      "        -0.00000000e+00, -3.44811583e-12,  0.00000000e+00]]), 'dW5': array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [-3.60946214e-17,  0.00000000e+00, -4.80418620e-16,\n",
      "        -7.83702183e-17, -2.20940995e-16],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [-5.79919226e-17,  0.00000000e+00, -7.71871220e-16,\n",
      "        -1.25914595e-16, -3.54977906e-16],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00]]), 'db5': array([[ 0.00000000e+00,  0.00000000e+00, -7.75750055e-13,\n",
      "         0.00000000e+00, -1.24636955e-12,  0.00000000e+00]]), 'dX4': array([[ 2.19879300e-16,  5.16067337e-17, -8.15935166e-17,\n",
      "        -1.15637280e-16, -3.64354779e-16],\n",
      "       [ 2.48260244e-16,  6.02163512e-17, -1.21121033e-16,\n",
      "        -1.39950100e-16, -4.51449886e-16],\n",
      "       [-7.52456096e-16, -1.75394123e-16,  2.61203201e-16,\n",
      "         3.89892209e-16,  1.22196972e-15],\n",
      "       [-5.41662136e-16, -1.25678138e-16,  1.79385407e-16,\n",
      "         2.77868976e-16,  8.67701481e-16]]), 'dZ4': array([[ 2.19879300e-16,  0.00000000e+00, -8.15935166e-17,\n",
      "        -1.15637280e-16, -3.64354779e-16],\n",
      "       [ 2.48260244e-16,  0.00000000e+00, -1.21121033e-16,\n",
      "        -1.39950100e-16, -4.51449886e-16],\n",
      "       [-7.52456096e-16, -0.00000000e+00,  2.61203201e-16,\n",
      "         3.89892209e-16,  1.22196972e-15],\n",
      "       [-5.41662136e-16, -0.00000000e+00,  1.79385407e-16,\n",
      "         2.77868976e-16,  8.67701481e-16]]), 'dW4': array([[ 0.00000000e+00,  0.00000000e+00, -3.23549531e-20,\n",
      "        -1.78291470e-20],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  9.31792080e-21,\n",
      "         5.13462588e-21],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  1.61455305e-20,\n",
      "         8.89696971e-21],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  4.98994616e-20,\n",
      "         2.74970214e-20]]), 'db4': array([[-2.06494672e-16,  0.00000000e+00,  5.94685146e-17,\n",
      "         1.03043451e-16,  3.18466633e-16]]), 'dX3': array([[ 7.24764494e-20, -4.04240372e-20,  1.81684711e-20,\n",
      "        -3.76067944e-20],\n",
      "       [ 8.24436941e-20, -5.21768263e-20,  2.43465424e-20,\n",
      "        -4.44576184e-20],\n",
      "       [-2.47643450e-19,  1.34274940e-19, -5.97927590e-20,\n",
      "         1.27454489e-19],\n",
      "       [-1.78085769e-19,  9.47108162e-20, -4.18996864e-20,\n",
      "         9.11539869e-20]]), 'dZ3': array([[ 0.00000000e+00, -0.00000000e+00,  1.81684711e-20,\n",
      "        -3.76067944e-20],\n",
      "       [ 0.00000000e+00, -0.00000000e+00,  2.43465424e-20,\n",
      "        -4.44576184e-20],\n",
      "       [-0.00000000e+00,  0.00000000e+00, -5.97927590e-20,\n",
      "         1.27454489e-19],\n",
      "       [-0.00000000e+00,  0.00000000e+00, -4.18996864e-20,\n",
      "         9.11539869e-20]]), 'dW3': array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "       [-4.48137227e-24, -2.25661878e-24,  0.00000000e+00],\n",
      "       [ 1.03401855e-23,  5.20682466e-24,  0.00000000e+00]]), 'db3': array([[ 0.00000000e+00,  0.00000000e+00, -1.47943580e-20,\n",
      "         3.41360158e-20]]), 'dX2': array([[-3.38555131e-24,  1.42705149e-24, -2.87283891e-24],\n",
      "       [-4.12300743e-24,  1.33981885e-24, -3.30680650e-24],\n",
      "       [ 1.13990686e-23, -5.05225005e-24,  9.79198478e-24],\n",
      "       [ 8.11614721e-24, -3.71782310e-24,  7.03001888e-24]]), 'dZ2': array([[-3.38555131e-24,  1.42705149e-24, -0.00000000e+00],\n",
      "       [-4.12300743e-24,  1.33981885e-24, -0.00000000e+00],\n",
      "       [ 1.13990686e-23, -5.05225005e-24,  0.00000000e+00],\n",
      "       [ 8.11614721e-24, -3.71782310e-24,  0.00000000e+00]]), 'dW2': array([[ 2.63305349e-28,  1.11844032e-27],\n",
      "       [-2.56698781e-28, -7.13594263e-28],\n",
      "       [ 0.00000000e+00,  0.00000000e+00]]), 'db2': array([[ 3.00166427e-24, -1.50080070e-24,  0.00000000e+00]]), 'dX1': array([[-4.75814425e-28,  3.52767682e-28],\n",
      "       [-5.14859731e-28,  3.81000764e-28],\n",
      "       [ 1.64220305e-27, -1.21796981e-27],\n",
      "       [ 1.18882322e-27, -8.81923750e-28]]), 'dZ1': array([[-0.00000000e+00,  3.52767682e-28],\n",
      "       [-5.14859731e-28,  3.81000764e-28],\n",
      "       [ 0.00000000e+00, -1.21796981e-27],\n",
      "       [ 1.18882322e-27, -8.81923750e-28]]), 'dW1': array([[2.24969984e-28],\n",
      "       [3.05354002e-28]]), 'db1': array([[ 1.68490873e-28, -3.41531277e-28]])}\n"
     ]
    }
   ],
   "source": [
    "def d_relu(x):\n",
    "    dydx = np.ones_like(x)\n",
    "    dydx[x < 0] = 0\n",
    "    return dydx\n",
    "\n",
    "def grad(predictions, labels, params, cache):\n",
    "    num_layers = int(len(params) / 2)\n",
    "    grads = {'dZ' + str(num_layers): predictions - labels}\n",
    "    for i in reversed(range(num_layers)):\n",
    "        grads['dW' + str(i+1)] = grads['dZ' + str(i+1)].T @ cache['X' + str(i)]\n",
    "        grads['db' + str(i+1)] = np.mean(grads['dZ' + str(i+1)], axis=0, keepdims=True)\n",
    "        if i==0:\n",
    "            break\n",
    "        grads['dX' + str(i)] = grads['dZ' + str(i+1)] @ params['W' + str(i+1)]\n",
    "        grads['dZ' + str(i)] = grads['dX' + str(i)] * d_relu(cache['Z' + str(i)])\n",
    "\n",
    "    return grads\n",
    "\n",
    "# Sanity check\n",
    "dummy_grad = grad(dummy_preds, dummy_labels, dummy_params, dummy_cache)\n",
    "print(dummy_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Gradient Descent Model Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations 1: training loss: 1.386289275928469, testing loss: 1.3862894458947372\n",
      "Iterations 2: training loss: 1.3862880547666538, testing loss: 1.3862881881729092\n",
      "Iterations 3: training loss: 1.3862867383095654, testing loss: 1.3862868347415198\n",
      "Iterations 4: training loss: 1.3862850752535854, testing loss: 1.386285120022894\n",
      "Iterations 5: training loss: 1.3862827914406206, testing loss: 1.3862827476281028\n",
      "Iterations 6: training loss: 1.3862795446655582, testing loss: 1.3862793490341114\n",
      "Iterations 7: training loss: 1.3862748365039512, testing loss: 1.3862743923214136\n",
      "Iterations 8: training loss: 1.3862679534540445, testing loss: 1.386267097406837\n",
      "Iterations 9: training loss: 1.386257846405468, testing loss: 1.386256316361708\n",
      "Iterations 10: training loss: 1.3862429340956057, testing loss: 1.386240340882599\n",
      "Iterations 11: training loss: 1.3862208723376142, testing loss: 1.3862166267460763\n",
      "Iterations 12: training loss: 1.3861881728374792, testing loss: 1.386181383160496\n",
      "Iterations 13: training loss: 1.3861396460622006, testing loss: 1.3861289563923012\n",
      "Iterations 14: training loss: 1.3860675978024097, testing loss: 1.3860509619800858\n",
      "Iterations 15: training loss: 1.3859606325706708, testing loss: 1.3859349711389297\n",
      "Iterations 16: training loss: 1.3858019576761917, testing loss: 1.3857626514951098\n",
      "Iterations 17: training loss: 1.3855669659498657, testing loss: 1.385507127757714\n",
      "Iterations 18: training loss: 1.385219923933239, testing loss: 1.385129350088613\n",
      "Iterations 19: training loss: 1.3847096979467644, testing loss: 1.384573414191737\n",
      "Iterations 20: training loss: 1.383964760536344, testing loss: 1.383761077831937\n",
      "Iterations 21: training loss: 1.3828885425811812, testing loss: 1.3825866674363636\n",
      "Iterations 22: training loss: 1.3813580086394421, testing loss: 1.3809155070029537\n",
      "Iterations 23: training loss: 1.3792314422755363, testing loss: 1.3785924672094811\n",
      "Iterations 24: training loss: 1.376375533143238, testing loss: 1.3754717669271226\n",
      "Iterations 25: training loss: 1.3727233943274364, testing loss: 1.3714809593658055\n",
      "Iterations 26: training loss: 1.368364081244032, testing loss: 1.366720138736763\n",
      "Iterations 27: training loss: 1.363625948443859, testing loss: 1.3615555083179065\n",
      "Iterations 28: training loss: 1.3590592587106916, testing loss: 1.3566032544673257\n",
      "Iterations 29: training loss: 1.355230876850744, testing loss: 1.3525054792475595\n",
      "Iterations 30: training loss: 1.3524210976210373, testing loss: 1.3495926612815832\n",
      "Iterations 31: training loss: 1.3505069613869267, testing loss: 1.3477447829440694\n",
      "Iterations 32: training loss: 1.3491500089112562, testing loss: 1.3465898749880987\n",
      "Iterations 33: training loss: 1.3480463095420578, testing loss: 1.3457822875082364\n",
      "Iterations 34: training loss: 1.3470173791190152, testing loss: 1.3451124368762364\n",
      "Iterations 35: training loss: 1.3459787131940573, testing loss: 1.3444786936119866\n",
      "Iterations 36: training loss: 1.3448910027229055, testing loss: 1.3438348206200974\n",
      "Iterations 37: training loss: 1.3437318618473717, testing loss: 1.3431574065449507\n",
      "Iterations 38: training loss: 1.3424837637985159, testing loss: 1.3424310711506182\n",
      "Iterations 39: training loss: 1.3411295390859397, testing loss: 1.341642215056246\n",
      "Iterations 40: training loss: 1.3396506031082145, testing loss: 1.3407766027840566\n",
      "Iterations 41: training loss: 1.3380261361875512, testing loss: 1.3398186807746935\n",
      "Iterations 42: training loss: 1.3362342078160465, testing loss: 1.3387528641498054\n",
      "Iterations 43: training loss: 1.3342484756907793, testing loss: 1.337560170777383\n",
      "Iterations 44: training loss: 1.332041303880066, testing loss: 1.336220588836455\n",
      "Iterations 45: training loss: 1.329577997905401, testing loss: 1.3347069827707414\n",
      "Iterations 46: training loss: 1.3268403910286262, testing loss: 1.3330112487694927\n",
      "Iterations 47: training loss: 1.3238104447156347, testing loss: 1.3311359793859876\n",
      "Iterations 48: training loss: 1.320460752344534, testing loss: 1.3290715474315038\n",
      "Iterations 49: training loss: 1.316765617923041, testing loss: 1.3267847452684958\n",
      "Iterations 50: training loss: 1.3127296953975407, testing loss: 1.3243112727136352\n",
      "Iterations 51: training loss: 1.308414462539423, testing loss: 1.3217166271047642\n",
      "Iterations 52: training loss: 1.3037985361817315, testing loss: 1.319013893394788\n",
      "Iterations 53: training loss: 1.2989071113611221, testing loss: 1.316156527378929\n",
      "Iterations 54: training loss: 1.293877774589957, testing loss: 1.3133108770259192\n",
      "Iterations 55: training loss: 1.2886826038910169, testing loss: 1.3104278021369176\n",
      "Iterations 56: training loss: 1.2833608874829774, testing loss: 1.30742403194916\n",
      "Iterations 57: training loss: 1.2779656278772624, testing loss: 1.3043972993541126\n",
      "Iterations 58: training loss: 1.2724673890665394, testing loss: 1.3011862572733293\n",
      "Iterations 59: training loss: 1.2667379503205587, testing loss: 1.2976225497322014\n",
      "Iterations 60: training loss: 1.2606928575135754, testing loss: 1.2936345605166923\n",
      "Iterations 61: training loss: 1.2542560715018845, testing loss: 1.2891649422834568\n",
      "Iterations 62: training loss: 1.247370591013817, testing loss: 1.2841472644516767\n",
      "Iterations 63: training loss: 1.2400066912470535, testing loss: 1.2786315413703013\n",
      "Iterations 64: training loss: 1.2321638085981141, testing loss: 1.2725770906340723\n",
      "Iterations 65: training loss: 1.223871005976711, testing loss: 1.2661677946624637\n",
      "Iterations 66: training loss: 1.2151808952935088, testing loss: 1.259399703089536\n",
      "Iterations 67: training loss: 1.206153720636451, testing loss: 1.2524632111600391\n",
      "Iterations 68: training loss: 1.1968537049494912, testing loss: 1.2453093248271516\n",
      "Iterations 69: training loss: 1.1873612282553765, testing loss: 1.2382154105123533\n",
      "Iterations 70: training loss: 1.1777642333722447, testing loss: 1.2310112725504612\n",
      "Iterations 71: training loss: 1.168143095589073, testing loss: 1.2240407012796937\n",
      "Iterations 72: training loss: 1.1585754500372345, testing loss: 1.2169642611577418\n",
      "Iterations 73: training loss: 1.1491642190970448, testing loss: 1.2103347782305083\n",
      "Iterations 74: training loss: 1.1400397630244983, testing loss: 1.2033131056740307\n",
      "Iterations 75: training loss: 1.131147841304365, testing loss: 1.197460795393264\n",
      "Iterations 76: training loss: 1.1224852961686689, testing loss: 1.1907237686137109\n",
      "Iterations 77: training loss: 1.114048384837833, testing loss: 1.185557578480178\n",
      "Iterations 78: training loss: 1.1058292941898489, testing loss: 1.1782511908762237\n",
      "Iterations 79: training loss: 1.0978364087465726, testing loss: 1.17453391940608\n",
      "Iterations 80: training loss: 1.0901227406010212, testing loss: 1.165166268717455\n",
      "Iterations 81: training loss: 1.0829233262520217, testing loss: 1.1655785159261707\n",
      "Iterations 82: training loss: 1.0768656195872794, testing loss: 1.1507022099035111\n",
      "Iterations 83: training loss: 1.0741271599217082, testing loss: 1.1680543123441498\n",
      "Iterations 84: training loss: 1.0822931926899721, testing loss: 1.1433893922576044\n",
      "Iterations 85: training loss: 1.1188123951025148, testing loss: 1.2433810687879703\n",
      "Iterations 86: training loss: 1.2248557116950716, testing loss: 1.2441673982153532\n",
      "Iterations 87: training loss: 1.3153722597885176, testing loss: 1.4879122652355878\n",
      "Iterations 88: training loss: 1.3903653979683197, testing loss: 1.3766492114070323\n",
      "Iterations 89: training loss: 1.1507135449637578, testing loss: 1.269755108901261\n",
      "Iterations 90: training loss: 1.1051770688489198, testing loss: 1.154725511815137\n",
      "Iterations 91: training loss: 1.0913789018445046, testing loss: 1.1921805641348124\n",
      "Iterations 92: training loss: 1.09430624881063, testing loss: 1.1708256387302127\n",
      "Iterations 93: training loss: 1.0967908451172466, testing loss: 1.1821979010954302\n",
      "Iterations 94: training loss: 1.1544366054100603, testing loss: 1.2603297426671678\n",
      "Iterations 95: training loss: 1.060225819157133, testing loss: 1.1275483183111792\n",
      "Iterations 96: training loss: 1.0758517634523888, testing loss: 1.1878169522209743\n",
      "Iterations 97: training loss: 1.0805856859467982, testing loss: 1.1400712785583385\n",
      "Iterations 98: training loss: 1.1048062424221206, testing loss: 1.2279283356251045\n",
      "Iterations 99: training loss: 1.0690444617502246, testing loss: 1.1261223871860326\n",
      "Iterations 100: training loss: 1.0479949120153447, testing loss: 1.1634021094251932\n",
      "Iterations 101: training loss: 1.0545416690298615, testing loss: 1.1138186305496776\n",
      "Iterations 102: training loss: 1.060244071652491, testing loss: 1.182128323182797\n",
      "Iterations 103: training loss: 1.0561719232048536, testing loss: 1.1135594804748563\n",
      "Iterations 104: training loss: 1.0567324807864424, testing loss: 1.1795993649288459\n",
      "Iterations 105: training loss: 1.0322947909048086, testing loss: 1.0930804539325574\n",
      "Iterations 106: training loss: 1.012752941983484, testing loss: 1.1257473449529636\n",
      "Iterations 107: training loss: 1.0122053566939138, testing loss: 1.0782814763580628\n",
      "Iterations 108: training loss: 1.0246526063542458, testing loss: 1.1412180040720648\n",
      "Iterations 109: training loss: 1.0185420072323366, testing loss: 1.08732961330813\n",
      "Iterations 110: training loss: 1.0422284195010099, testing loss: 1.1533457379312446\n",
      "Iterations 111: training loss: 1.0275349313222637, testing loss: 1.1197975968452045\n",
      "Iterations 112: training loss: 1.1192889733212292, testing loss: 1.2071170251796615\n",
      "Iterations 113: training loss: 1.1534392543956042, testing loss: 1.3019890280175168\n",
      "Iterations 114: training loss: 1.2786612733557436, testing loss: 1.3149632589310647\n",
      "Iterations 115: training loss: 1.2895466542957652, testing loss: 1.4489989819988123\n",
      "Iterations 116: training loss: 1.0779354651256106, testing loss: 1.1072297150652468\n",
      "Iterations 117: training loss: 0.9991140752822859, testing loss: 1.0769977937029647\n",
      "Iterations 118: training loss: 0.9773006478088219, testing loss: 1.0504328761021546\n",
      "Iterations 119: training loss: 0.9724009233692605, testing loss: 1.072299594157361\n",
      "Iterations 120: training loss: 0.9964875089616002, testing loss: 1.0744563349243408\n",
      "Iterations 121: training loss: 1.067507064477476, testing loss: 1.1874121188880984\n",
      "Iterations 122: training loss: 1.0102005393864126, testing loss: 1.0851753731866827\n",
      "Iterations 123: training loss: 1.0457622613211712, testing loss: 1.1612266111118612\n",
      "Iterations 124: training loss: 0.9618818400557578, testing loss: 1.0458116135033915\n",
      "Iterations 125: training loss: 0.961377785547227, testing loss: 1.0557553242469495\n",
      "Iterations 126: training loss: 0.9614779035314416, testing loss: 1.0600928228722466\n",
      "Iterations 127: training loss: 1.0227664106535614, testing loss: 1.1112185276959559\n",
      "Iterations 128: training loss: 1.0266644641234164, testing loss: 1.1501348922485342\n",
      "Iterations 129: training loss: 1.1182417642986309, testing loss: 1.1849475232214775\n",
      "Iterations 130: training loss: 1.1215051891923866, testing loss: 1.263747278083556\n",
      "Iterations 131: training loss: 1.0518492797788648, testing loss: 1.0931220233131311\n",
      "Iterations 132: training loss: 1.0362955232248865, testing loss: 1.1520252198454362\n",
      "Iterations 133: training loss: 0.9643455371276684, testing loss: 1.0162294937296776\n",
      "Iterations 134: training loss: 0.9228936909295784, testing loss: 1.0202850997707216\n",
      "Iterations 135: training loss: 0.9256641598131296, testing loss: 0.9954578509436675\n",
      "Iterations 136: training loss: 0.9587556006082604, testing loss: 1.0731572968707868\n",
      "Iterations 137: training loss: 1.0257147661777184, testing loss: 1.0824879215793202\n",
      "Iterations 138: training loss: 1.0944947958351634, testing loss: 1.2349411808688853\n",
      "Iterations 139: training loss: 1.0124582181955208, testing loss: 1.0702579805178754\n",
      "Iterations 140: training loss: 0.945344393928877, testing loss: 1.0507076021441077\n",
      "Iterations 141: training loss: 0.91715831314071, testing loss: 0.9963486018287954\n",
      "Iterations 142: training loss: 0.9204592796404539, testing loss: 1.0188935020611996\n",
      "Iterations 143: training loss: 0.928961370114867, testing loss: 1.021943635465908\n",
      "Iterations 144: training loss: 1.014049266654312, testing loss: 1.1124892676133324\n",
      "Iterations 145: training loss: 1.013300939375543, testing loss: 1.133417553567935\n",
      "Iterations 146: training loss: 1.1650560585434584, testing loss: 1.2439598359538044\n",
      "Iterations 147: training loss: 1.0879391693179037, testing loss: 1.2223674581776771\n",
      "Iterations 148: training loss: 0.9873121612850186, testing loss: 1.0346103020988895\n",
      "Iterations 149: training loss: 0.944643819585007, testing loss: 1.0480831511677615\n",
      "Iterations 150: training loss: 0.9071204922736309, testing loss: 0.9750510095869678\n",
      "Iterations 151: training loss: 0.8945965613399471, testing loss: 0.9933352890540618\n",
      "Iterations 152: training loss: 0.9157657269718539, testing loss: 0.9951690715640524\n",
      "Iterations 153: training loss: 0.9661950296118433, testing loss: 1.0770093468970585\n",
      "Iterations 154: training loss: 1.0816708486654374, testing loss: 1.1738099933478665\n",
      "Iterations 155: training loss: 0.9573095529000626, testing loss: 1.0632302707327834\n",
      "Iterations 156: training loss: 1.0088567736690603, testing loss: 1.0972596418841634\n",
      "Iterations 157: training loss: 0.90550952401347, testing loss: 1.0052777126045163\n",
      "Iterations 158: training loss: 0.9251424037709793, testing loss: 1.0084604539646715\n",
      "Iterations 159: training loss: 0.9054031703273923, testing loss: 1.0088341470630686\n",
      "Iterations 160: training loss: 0.9391465700216658, testing loss: 1.0251503817158791\n",
      "Iterations 161: training loss: 0.9152250500220168, testing loss: 1.0191624076043269\n",
      "Iterations 162: training loss: 0.9594117792363616, testing loss: 1.0518958227615447\n",
      "Iterations 163: training loss: 0.9090175694793456, testing loss: 1.0048958754908617\n",
      "Iterations 164: training loss: 0.9568837347752811, testing loss: 1.0621442193367678\n",
      "Iterations 165: training loss: 0.9246735618427223, testing loss: 0.9961607076806794\n",
      "Iterations 166: training loss: 1.0033420854952424, testing loss: 1.1416469710996975\n",
      "Iterations 167: training loss: 1.0744691205134373, testing loss: 1.098385926247995\n",
      "Iterations 168: training loss: 1.0467088823395818, testing loss: 1.1940975880934692\n",
      "Iterations 169: training loss: 0.992201187054993, testing loss: 1.0326306221346204\n",
      "Iterations 170: training loss: 0.9027485402603933, testing loss: 1.0042837746318134\n",
      "Iterations 171: training loss: 0.8712597681687899, testing loss: 0.9459327872351384\n",
      "Iterations 172: training loss: 0.8604652637626387, testing loss: 0.9640225543125429\n",
      "Iterations 173: training loss: 0.8811918995436467, testing loss: 0.9649750707086636\n",
      "Iterations 174: training loss: 0.9174585832735466, testing loss: 1.0325181015461005\n",
      "Iterations 175: training loss: 1.0089778187797431, testing loss: 1.1007900891790077\n",
      "Iterations 176: training loss: 0.9620031854003858, testing loss: 1.0786952783111716\n",
      "Iterations 177: training loss: 1.0193865442196546, testing loss: 1.1132186029283357\n",
      "Iterations 178: training loss: 0.871481350273464, testing loss: 0.9726329088387818\n",
      "Iterations 179: training loss: 0.872854016934276, testing loss: 0.9565438303043512\n",
      "Iterations 180: training loss: 0.8551100486964335, testing loss: 0.9589010890843527\n",
      "Iterations 181: training loss: 0.8694942396446751, testing loss: 0.9522797020073483\n",
      "Iterations 182: training loss: 0.8707370126595408, testing loss: 0.9814971896646564\n",
      "Iterations 183: training loss: 0.9005314621875152, testing loss: 0.9799625984840987\n",
      "Iterations 184: training loss: 0.9070228034051601, testing loss: 1.0268966504796597\n",
      "Iterations 185: training loss: 0.9333145976573656, testing loss: 1.0057754576894042\n",
      "Iterations 186: training loss: 0.9329374974452599, testing loss: 1.0591153621768568\n",
      "Iterations 187: training loss: 0.9096610833794019, testing loss: 0.9690281620973097\n",
      "Iterations 188: training loss: 0.940585156085231, testing loss: 1.0678760104351643\n",
      "Iterations 189: training loss: 0.9267197610287362, testing loss: 0.9705603536150913\n",
      "Iterations 190: training loss: 1.0247812482774867, testing loss: 1.1658076151432826\n",
      "Iterations 191: training loss: 1.1740445766480003, testing loss: 1.2121123146088688\n",
      "Iterations 192: training loss: 0.9309605657467301, testing loss: 1.025935064134841\n",
      "Iterations 193: training loss: 0.8541905606095054, testing loss: 0.9454105602927216\n",
      "Iterations 194: training loss: 0.8203732649439586, testing loss: 0.9134604905743602\n",
      "Iterations 195: training loss: 0.8116783882530365, testing loss: 0.9108957999271666\n",
      "Iterations 196: training loss: 0.8187428657781919, testing loss: 0.9113807110713908\n",
      "Iterations 197: training loss: 0.8370636803091439, testing loss: 0.9430556312815487\n",
      "Iterations 198: training loss: 0.8977201340643624, testing loss: 0.997252835779111\n",
      "Iterations 199: training loss: 0.9345045120367482, testing loss: 1.0447638019445278\n",
      "Iterations 200: training loss: 1.060427437056995, testing loss: 1.1825680229197726\n",
      "Iterations 201: training loss: 0.8830334150705217, testing loss: 0.9659027192183486\n",
      "Iterations 202: training loss: 0.8546491990607639, testing loss: 0.9667780311412973\n",
      "Iterations 203: training loss: 0.8504466413110268, testing loss: 0.9223797972058367\n",
      "Iterations 204: training loss: 0.8804177658101703, testing loss: 1.0105050341176254\n",
      "Iterations 205: training loss: 0.9399135366385166, testing loss: 0.9876972597726925\n",
      "Iterations 206: training loss: 0.9830754383344235, testing loss: 1.1262023397285397\n",
      "Iterations 207: training loss: 0.9351888955698214, testing loss: 0.9875193678065562\n",
      "Iterations 208: training loss: 0.8189722165293776, testing loss: 0.9189708954188286\n",
      "Iterations 209: training loss: 0.791456310593927, testing loss: 0.8728129785135844\n",
      "Iterations 210: training loss: 0.7773297383136916, testing loss: 0.8796125459589733\n",
      "Iterations 211: training loss: 0.776199702296408, testing loss: 0.8594982841980825\n",
      "Iterations 212: training loss: 0.7893911063809391, testing loss: 0.9027120744254701\n",
      "Iterations 213: training loss: 0.8175425566410028, testing loss: 0.8890209303532323\n",
      "Iterations 214: training loss: 0.9108806599571059, testing loss: 1.0469015350024158\n",
      "Iterations 215: training loss: 0.9553363539649554, testing loss: 1.0249287744812567\n",
      "Iterations 216: training loss: 1.0333760195313588, testing loss: 1.1634788882464533\n",
      "Iterations 217: training loss: 1.0315728416056686, testing loss: 1.1296225498732078\n",
      "Iterations 218: training loss: 0.9201080732149013, testing loss: 0.9752544922697328\n",
      "Iterations 219: training loss: 0.9790896011919232, testing loss: 1.1132185089239013\n",
      "Iterations 220: training loss: 0.9948004781478563, testing loss: 1.0512329869622627\n",
      "Iterations 221: training loss: 0.8578488614905704, testing loss: 0.9653013897467713\n",
      "Iterations 222: training loss: 0.7912627285631063, testing loss: 0.8903284182014803\n",
      "Iterations 223: training loss: 0.7709327623120867, testing loss: 0.8691673463034005\n",
      "Iterations 224: training loss: 0.7647294819139377, testing loss: 0.8680820483644778\n",
      "Iterations 225: training loss: 0.7644080464179307, testing loss: 0.8570527797447752\n",
      "Iterations 226: training loss: 0.7713874512128696, testing loss: 0.8832521969498096\n",
      "Iterations 227: training loss: 0.7973650269544674, testing loss: 0.8787056476648233\n",
      "Iterations 228: training loss: 0.863795324321888, testing loss: 1.0003105618385635\n",
      "Iterations 229: training loss: 0.9708111445812037, testing loss: 1.0441524250617313\n",
      "Iterations 230: training loss: 1.0889930453512706, testing loss: 1.2398906387924362\n",
      "Iterations 231: training loss: 1.0144027378162286, testing loss: 1.109603428512648\n",
      "Iterations 232: training loss: 0.8237305138883338, testing loss: 0.8940760291943056\n",
      "Iterations 233: training loss: 0.8292134534917188, testing loss: 0.9363778860793147\n",
      "Iterations 234: training loss: 0.8702789251641956, testing loss: 0.9380342869990437\n",
      "Iterations 235: training loss: 0.9351577812571706, testing loss: 1.0795859877622815\n",
      "Iterations 236: training loss: 1.0006371808685295, testing loss: 1.051019617672402\n",
      "Iterations 237: training loss: 0.8447378230786872, testing loss: 0.9615673670254028\n",
      "Iterations 238: training loss: 0.7865729719133584, testing loss: 0.8720650998771126\n",
      "Iterations 239: training loss: 0.7587108815495752, testing loss: 0.8680131354650918\n",
      "Iterations 240: training loss: 0.7526667454462072, testing loss: 0.8435071061320423\n",
      "Iterations 241: training loss: 0.753014805120267, testing loss: 0.866092522181743\n",
      "Iterations 242: training loss: 0.762025792422512, testing loss: 0.8437857128136056\n",
      "Iterations 243: training loss: 0.7909229241755974, testing loss: 0.9166883250638407\n",
      "Iterations 244: training loss: 0.821703854641001, testing loss: 0.8877113806684436\n",
      "Iterations 245: training loss: 0.8922626670691928, testing loss: 1.0329960222293826\n",
      "Iterations 246: training loss: 0.8653483470515282, testing loss: 0.9310082471666302\n",
      "Iterations 247: training loss: 0.8374602364819701, testing loss: 0.9581232982455085\n",
      "Iterations 248: training loss: 0.7933961056155289, testing loss: 0.8744089021053886\n",
      "Iterations 249: training loss: 0.7580813763530283, testing loss: 0.86252723769234\n",
      "Iterations 250: training loss: 0.7580996069855341, testing loss: 0.8566130650524237\n",
      "Iterations 251: training loss: 0.7786942520959994, testing loss: 0.8831300239184156\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m losses_train\u001b[38;5;241m.\u001b[39mappend(loss_train)\n\u001b[1;32m     14\u001b[0m losses_test\u001b[38;5;241m.\u001b[39mappend(loss_test)\n\u001b[0;32m---> 15\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(layer_sizes) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     17\u001b[0m     params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m-\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdW\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(predictions, labels, params, cache)\u001b[0m\n\u001b[1;32m      8\u001b[0m grads \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdZ\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(num_layers): predictions \u001b[38;5;241m-\u001b[39m labels}\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(num_layers)):\n\u001b[0;32m---> 10\u001b[0m     grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdW\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m] \u001b[38;5;241m=\u001b[39m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdZ\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)]\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m cache[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i)]\n\u001b[1;32m     11\u001b[0m     grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdZ\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "layer_sizes = [features_train.shape[1], 128, 4]  # structure\n",
    "params = init_params(layer_sizes)\n",
    "num_iters = 1000\n",
    "learning_rate = 1e-5  # 0.00001 \n",
    "losses_train, losses_test = [], []\n",
    "\n",
    "for i in range(num_iters):\n",
    "    preds_train, cache = forward(features_train, params)\n",
    "    preds_test, _ = forward(features_test, params)\n",
    "    loss_train = ce_loss(preds_train, labels_train)\n",
    "    loss_test = ce_loss(preds_test, labels_test)\n",
    "    print(f\"Iterations {i+1}: training loss: {loss_train}, testing loss: {loss_test}\")\n",
    "    losses_train.append(loss_train)\n",
    "    losses_test.append(loss_test)\n",
    "    grads = grad(preds_train, labels_train, params, cache)\n",
    "    for j in range(len(layer_sizes) - 1):\n",
    "        params['W' + str(j+1)] = params['W' + str(j+1)] - learning_rate * grads['dW' + str(j+1)]\n",
    "        params['b' + str(j+1)] = params['b' + str(j+1)] - learning_rate * grads['db' + str(j+1)]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(num_iters), losses_train, '--b', range(num_iters), losses_test, 'r')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 Accuracy Assess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27923669467787116\n"
     ]
    }
   ],
   "source": [
    "pred_classes_train = np.argmax(preds_train, axis=1)\n",
    "is_correct_train = pred_classes_train == raw_labels_train\n",
    "num_correct_train = sum(is_correct_train)\n",
    "accuracy_train = num_correct_train / len(raw_labels_train)\n",
    "print(accuracy_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3321",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
