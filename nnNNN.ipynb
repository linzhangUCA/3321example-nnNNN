{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify Brain MRI Images (Multi-Class Classification)\n",
    "\n",
    "## Pre-requisites\n",
    "Install [kagglehub](https://pypi.org/project/kagglehub/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load Dataset\n",
    "### 1.1 Download Data and Generate Annotation Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pbd0/miniforge3/envs/3321/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Download dataset and locate it in machine\n",
    "data_dirname = kagglehub.dataset_download(\"masoudnickparvar/brain-tumor-mri-dataset\")\n",
    "# print(data_dirname)\n",
    "train_dirname = os.path.join(data_dirname, 'Training')\n",
    "test_dirname = os.path.join(data_dirname, 'Testing')\n",
    "classes = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
    "\n",
    "# Get training files\n",
    "tr_gl_files = glob(os.path.join(train_dirname, classes[0], '*.jpg'))\n",
    "tr_me_files = glob(os.path.join(train_dirname, classes[1], '*.jpg'))\n",
    "tr_no_files = glob(os.path.join(train_dirname, classes[2], '*.jpg'))\n",
    "tr_pi_files = glob(os.path.join(train_dirname, classes[3], '*.jpg'))\n",
    "# print(len(tr_gl_files), len(tr_me_files), len(tr_no_files), len(tr_pi_files))\n",
    "train_files = tr_gl_files + tr_me_files + tr_no_files + tr_pi_files\n",
    "train_labels = [classes[0]] * len(tr_gl_files) + \\\n",
    "    [classes[1]] * len(tr_me_files) + \\\n",
    "    [classes[2]] * len(tr_no_files) + \\\n",
    "    [classes[-1]] * len(tr_pi_files)\n",
    "train_dict = {'path': train_files, 'label': train_labels}\n",
    "df_train = pd.DataFrame(train_dict)\n",
    "# print(df_train)\n",
    "df_train.to_csv('annotation_train.csv', header=False, index=False)\n",
    "\n",
    "# Get testing files\n",
    "te_gl_files = glob(os.path.join(test_dirname, classes[0], '*.jpg'))\n",
    "te_me_files = glob(os.path.join(test_dirname, classes[1], '*.jpg'))\n",
    "te_no_files = glob(os.path.join(test_dirname, classes[2], '*.jpg'))\n",
    "te_pi_files = glob(os.path.join(test_dirname, classes[3], '*.jpg'))\n",
    "# print(len(te_gl_files), len(te_me_files), len(te_no_files), len(te_pi_files))\n",
    "test_files = te_gl_files + te_me_files + te_no_files + te_pi_files\n",
    "test_labels = [classes[0]] * len(te_gl_files) + \\\n",
    "    [classes[1]] * len(te_me_files) + \\\n",
    "    [classes[2]] * len(te_no_files) + \\\n",
    "    [classes[-1]] * len(te_pi_files)\n",
    "test_dict = {'path': test_files, 'label': test_labels}\n",
    "df_test = pd.DataFrame(test_dict)\n",
    "# print(df_train)\n",
    "df_test.to_csv('annotation_test.csv', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "\n",
    "class TumorDataset(Dataset):\n",
    "    def __init__(self, annotations_file):\n",
    "        self.imgs_info = pd.read_csv(annotations_file, header=None)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_info)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_path = self.imgs_info.iloc[idx, 0]\n",
    "        img_raw = cv.imread(img_path, cv.IMREAD_GRAYSCALE)\n",
    "        image = cv.resize(img_raw, (128, 128))\n",
    "        if self.imgs_info.iloc[idx, 1] == classes[0]:\n",
    "            category = 0\n",
    "        elif self.imgs_info.iloc[idx, 1] == classes[1]:\n",
    "            category = 1\n",
    "        elif self.imgs_info.iloc[idx, 1] == classes[2]:\n",
    "            category = 2\n",
    "        else:\n",
    "            category = 3\n",
    "        sample = {'image': image, 'category': category}\n",
    "        return sample\n",
    "    \n",
    "dataset_train = TumorDataset(annotations_file='annotation_train.csv')\n",
    "# for i, sample in enumerate(dataset_train):\n",
    "#     image = sample['image']\n",
    "#     label = sample['category']\n",
    "#     if not i%100:  # i % 100 != 0\n",
    "#         print(i, image.shape, label)\n",
    "# print(i, image.shape, label)\n",
    "dataset_test = TumorDataset(annotations_file='annotation_test.csv')\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=10000, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=10000, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Pre-Process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5712, 128, 128) (5712,)\n",
      "(1311, 128, 128) (1311,)\n",
      "(5712, 16384) (5712, 4)\n",
      "(1311, 16384) (1311, 4)\n",
      "[3 3 1 0]\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Extract features and labels from the dataloaders\n",
    "data_train = next(iter(dataloader_train))\n",
    "data_test = next(iter(dataloader_test))\n",
    "\n",
    "# Separate features and labels\n",
    "raw_features_train = data_train['image'].numpy()\n",
    "raw_features_test = data_test['image'].numpy()\n",
    "raw_labels_train = data_train['category'].numpy()\n",
    "raw_labels_test = data_test['category'].numpy()\n",
    "print(raw_features_train.shape, raw_labels_train.shape)\n",
    "print(raw_features_test.shape, raw_labels_test.shape)\n",
    "\n",
    "# Reshape\n",
    "reshaped_features_train = raw_features_train.reshape(raw_features_train.shape[0], -1)\n",
    "reshaped_features_test = raw_features_test.reshape(raw_features_test.shape[0], -1)\n",
    "onehot_labels_train = np.zeros((raw_labels_train.shape[0], len(classes)))\n",
    "onehot_labels_test = np.zeros((raw_labels_test.shape[0], len(classes)))\n",
    "print(reshaped_features_train.shape, onehot_labels_train.shape)\n",
    "print(reshaped_features_test.shape, onehot_labels_test.shape)\n",
    "\n",
    "# One-hot encoding\n",
    "onehot_labels_train[np.arange(raw_labels_train.shape[0]), raw_labels_train] = 1\n",
    "onehot_labels_test[np.arange(raw_labels_test.shape[0]), raw_labels_test] = 1\n",
    "print(raw_labels_train[:4])\n",
    "print(onehot_labels_train[:4])\n",
    "\n",
    "# Rescale\n",
    "rescaled_features_train = reshaped_features_train / 255\n",
    "rescaled_features_test = reshaped_features_test / 255\n",
    "# print(rescaled_features_test[0, 8000:8500])\n",
    "\n",
    "features_train = rescaled_features_train\n",
    "features_test = rescaled_features_test\n",
    "labels_train = onehot_labels_train\n",
    "labels_test = onehot_labels_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Build Multi-Layer Perceptron Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "[[0.11110751 0.11109948 0.11110439 0.11110372 0.11112242 0.11111873\n",
      "  0.11111797 0.11110921 0.11111657]\n",
      " [0.11110751 0.11109948 0.11110439 0.11110372 0.11112242 0.11111873\n",
      "  0.11111797 0.11110921 0.11111657]\n",
      " [0.11110751 0.11109948 0.11110439 0.11110372 0.11112242 0.11111873\n",
      "  0.11111797 0.11110921 0.11111657]\n",
      " [0.11110751 0.11109948 0.11110439 0.11110372 0.11112242 0.11111873\n",
      "  0.11111797 0.11110921 0.11111657]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def init_params(layer_sizes):\n",
    "    \"\"\"\n",
    "    layer_sizes: list/tuple\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    for i in range(len(layer_sizes)-1):\n",
    "        parameters['W' + str(i+1)] = np.random.normal(0, 0.0001, size=(layer_sizes[i+1], layer_sizes[i]))\n",
    "        parameters['b' + str(i+1)] = np.random.normal(0, 0.0001, size=(1, layer_sizes[i+1]))\n",
    "    return parameters\n",
    "\n",
    "# Sanity check\n",
    "dummy_layer_sizes = list(range(1, 10))\n",
    "dummy_params = init_params(dummy_layer_sizes)\n",
    "print(len(dummy_params))\n",
    "\n",
    "def linear(in_features, weight, bias):\n",
    "    return in_features @ weight.T + bias\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(out_features):\n",
    "    probs = np.exp(out_features) / np.sum(np.exp(out_features), axis=1, keepdims=True)\n",
    "    return probs\n",
    "\n",
    "def forward(features_input, params):\n",
    "    num_layers = int(len(params) / 2 - 1)\n",
    "    cache = {'X0': features_input}\n",
    "    for i in range(num_layers):\n",
    "        cache['Z' + str(i+1)] = linear(cache['X' + str(i)], params['W' + str(i+1)], params['b' + str(i+1)])\n",
    "        cache['X' + str(i+1)] = relu(cache['Z' + str(i+1)])\n",
    "    cache['Z' + str(i+2)] = linear(cache['X' + str(i+1)], params['W' + str(i+2)], params['b' + str(i+2)])\n",
    "    predictions = softmax(cache['Z' + str(i+2)])\n",
    "\n",
    "    return predictions, cache\n",
    "\n",
    "# Sanity check\n",
    "dummy_features = np.random.normal(size=(4, 1))\n",
    "dummy_preds, dummy_cache = forward(dummy_features, dummy_params)\n",
    "print(dummy_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "2.1972067271091973\n"
     ]
    }
   ],
   "source": [
    "def ce_loss(predictions, labels):\n",
    "    error = -np.sum(labels * np.log(predictions), axis=1)\n",
    "    return np.mean(error)\n",
    "\n",
    "# Sanity check\n",
    "dummy_labels = np.zeros((4, 9))\n",
    "dummy_labels[np.arange(4), np.random.randint(0, 9, (4,))] = 1\n",
    "dummy_loss = ce_loss(dummy_preds, dummy_labels)\n",
    "print(dummy_labels)\n",
    "print(dummy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Back-Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dZ8': array([[ 0.11110751,  0.11109948,  0.11110439, -0.88889628,  0.11112242,\n",
      "         0.11111873,  0.11111797,  0.11110921,  0.11111657],\n",
      "       [ 0.11110751,  0.11109948,  0.11110439,  0.11110372, -0.88887758,\n",
      "         0.11111873,  0.11111797,  0.11110921,  0.11111657],\n",
      "       [ 0.11110751,  0.11109948,  0.11110439,  0.11110372,  0.11112242,\n",
      "        -0.88888127,  0.11111797,  0.11110921,  0.11111657],\n",
      "       [-0.88889249,  0.11109948,  0.11110439,  0.11110372,  0.11112242,\n",
      "         0.11111873,  0.11111797,  0.11110921,  0.11111657]]), 'dW8': array([[ 0.00000000e+00, -4.29209633e-05,  0.00000000e+00,\n",
      "        -2.41975624e-05,  0.00000000e+00,  0.00000000e+00,\n",
      "        -6.28952121e-06, -6.64021373e-05],\n",
      "       [ 0.00000000e+00,  3.43322870e-05,  0.00000000e+00,\n",
      "         1.93555221e-05,  0.00000000e+00,  0.00000000e+00,\n",
      "         5.03095995e-06,  5.31147733e-05],\n",
      "       [ 0.00000000e+00,  3.43338022e-05,  0.00000000e+00,\n",
      "         1.93563764e-05,  0.00000000e+00,  0.00000000e+00,\n",
      "         5.03118198e-06,  5.31171174e-05],\n",
      "       [ 0.00000000e+00, -4.29221330e-05,  0.00000000e+00,\n",
      "        -2.41982218e-05,  0.00000000e+00,  0.00000000e+00,\n",
      "        -6.28969261e-06, -6.64039468e-05],\n",
      "       [ 0.00000000e+00, -4.29163552e-05,  0.00000000e+00,\n",
      "        -2.41949644e-05,  0.00000000e+00,  0.00000000e+00,\n",
      "        -6.28884595e-06, -6.63950081e-05],\n",
      "       [ 0.00000000e+00, -4.29174955e-05,  0.00000000e+00,\n",
      "        -2.41956073e-05,  0.00000000e+00,  0.00000000e+00,\n",
      "        -6.28901304e-06, -6.63967723e-05],\n",
      "       [ 0.00000000e+00,  3.43379997e-05,  0.00000000e+00,\n",
      "         1.93587428e-05,  0.00000000e+00,  0.00000000e+00,\n",
      "         5.03179707e-06,  5.31236113e-05],\n",
      "       [ 0.00000000e+00,  3.43352924e-05,  0.00000000e+00,\n",
      "         1.93572165e-05,  0.00000000e+00,  0.00000000e+00,\n",
      "         5.03140036e-06,  5.31194229e-05],\n",
      "       [ 0.00000000e+00,  3.43375656e-05,  0.00000000e+00,\n",
      "         1.93584981e-05,  0.00000000e+00,  0.00000000e+00,\n",
      "         5.03173346e-06,  5.31229397e-05]]), 'db8': array([[-0.13889249,  0.11109948,  0.11110439, -0.13889628, -0.13887758,\n",
      "        -0.13888127,  0.11111797,  0.11110921,  0.11111657]]), 'dX7': array([[ 1.81137303e-05,  9.54741953e-05,  2.22636321e-04,\n",
      "        -1.37344217e-04,  1.45050031e-04,  3.08500607e-05,\n",
      "         9.04464754e-05,  3.78933104e-05],\n",
      "       [ 6.10621235e-05,  3.86264002e-05, -1.04662132e-05,\n",
      "        -2.03672623e-05,  1.02514481e-04, -5.56410119e-05,\n",
      "        -1.11241249e-04, -5.63657978e-05],\n",
      "       [ 5.32211805e-05, -3.92134781e-05,  1.64862311e-05,\n",
      "         5.52021519e-07, -6.47913790e-05, -5.49060479e-05,\n",
      "         4.24670958e-05, -2.10120705e-04],\n",
      "       [ 9.72906806e-05,  9.11189279e-05, -9.83624547e-05,\n",
      "         1.99252734e-05, -1.01796700e-04, -1.22829210e-04,\n",
      "         4.41209987e-05, -3.78824064e-05]]), 'dZ7': array([[ 0.00000000e+00,  9.54741953e-05,  0.00000000e+00,\n",
      "        -1.37344217e-04,  0.00000000e+00,  0.00000000e+00,\n",
      "         9.04464754e-05,  3.78933104e-05],\n",
      "       [ 0.00000000e+00,  3.86264002e-05, -0.00000000e+00,\n",
      "        -2.03672623e-05,  0.00000000e+00, -0.00000000e+00,\n",
      "        -1.11241249e-04, -5.63657978e-05],\n",
      "       [ 0.00000000e+00, -3.92134781e-05,  0.00000000e+00,\n",
      "         5.52021519e-07, -0.00000000e+00, -0.00000000e+00,\n",
      "         4.24670958e-05, -2.10120705e-04],\n",
      "       [ 0.00000000e+00,  9.11189279e-05, -0.00000000e+00,\n",
      "         1.99252734e-05, -0.00000000e+00, -0.00000000e+00,\n",
      "         4.41209987e-05, -3.78824064e-05]]), 'dW7': array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [ 0.00000000e+00,  1.93113205e-08,  1.92165719e-09,\n",
      "         4.21275364e-09,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [ 0.00000000e+00, -1.42477806e-08, -1.41778756e-09,\n",
      "        -3.10814528e-09,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [ 0.00000000e+00,  6.83072371e-09,  6.79720961e-10,\n",
      "         1.49011851e-09,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [ 0.00000000e+00, -2.76657444e-08, -2.75300058e-09,\n",
      "        -6.03526648e-09,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00]]), 'db7': array([[ 0.00000000e+00,  4.65015113e-05,  0.00000000e+00,\n",
      "        -3.43085462e-05,  0.00000000e+00,  0.00000000e+00,\n",
      "         1.64483302e-05, -6.66188998e-05]]), 'dX6': array([[-4.57473787e-09,  8.47869512e-09, -4.00676701e-09,\n",
      "        -2.09435499e-08,  4.17525235e-08, -1.56726024e-08,\n",
      "        -5.19439319e-09],\n",
      "       [ 1.99380574e-08,  1.71934382e-08, -3.45428066e-09,\n",
      "        -4.66626332e-09, -1.02310596e-08,  1.09134250e-08,\n",
      "         3.87801044e-08],\n",
      "       [-5.28643834e-08, -1.83310049e-08,  1.92851019e-08,\n",
      "         4.31204844e-08, -8.26994792e-09,  1.42046423e-08,\n",
      "         4.80711081e-08],\n",
      "       [ 6.52865517e-09, -8.50283623e-10,  6.03964571e-09,\n",
      "         2.05072970e-09,  5.11553035e-09, -4.54791782e-09,\n",
      "         8.48117277e-09]]), 'dZ6': array([[-0.00000000e+00,  8.47869512e-09, -4.00676701e-09,\n",
      "        -2.09435499e-08,  0.00000000e+00, -0.00000000e+00,\n",
      "        -0.00000000e+00],\n",
      "       [ 0.00000000e+00,  1.71934382e-08, -3.45428066e-09,\n",
      "        -4.66626332e-09, -0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [-0.00000000e+00, -1.83310049e-08,  1.92851019e-08,\n",
      "         4.31204844e-08, -0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [ 0.00000000e+00, -8.50283623e-10,  6.03964571e-09,\n",
      "         2.05072970e-09,  0.00000000e+00, -0.00000000e+00,\n",
      "         0.00000000e+00]]), 'dW6': array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 0.00000000e+00],\n",
      "       [1.34279573e-13, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 7.21815441e-13],\n",
      "       [3.69555900e-13, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 1.98653562e-12],\n",
      "       [4.04677144e-13, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 2.17532871e-12],\n",
      "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 0.00000000e+00],\n",
      "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 0.00000000e+00],\n",
      "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 0.00000000e+00]]), 'db6': array([[0.00000000e+00, 1.62271121e-09, 4.46592499e-09, 4.89035020e-09,\n",
      "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]), 'dX5': array([[-1.27614639e-12, -1.47304582e-13, -3.19592574e-12,\n",
      "         2.68179162e-12, -8.19969928e-14, -4.22140164e-12],\n",
      "       [-3.22489634e-12, -2.58477391e-12, -1.47218021e-12,\n",
      "         2.38732340e-12, -1.14057527e-12, -3.63722264e-12],\n",
      "       [ 2.95388576e-12,  7.22771605e-13,  6.13756990e-12,\n",
      "        -6.34985070e-12,  1.62864062e-13,  7.65340467e-12],\n",
      "       [ 2.10940498e-13,  1.59631683e-13,  6.20801307e-14,\n",
      "        -6.41274772e-13, -2.68181116e-14, -2.00962843e-13]]), 'dZ5': array([[-1.27614639e-12, -0.00000000e+00, -0.00000000e+00,\n",
      "         0.00000000e+00, -0.00000000e+00, -4.22140164e-12],\n",
      "       [-3.22489634e-12, -0.00000000e+00, -0.00000000e+00,\n",
      "         0.00000000e+00, -0.00000000e+00, -3.63722264e-12],\n",
      "       [ 2.95388576e-12,  0.00000000e+00,  0.00000000e+00,\n",
      "        -0.00000000e+00,  0.00000000e+00,  7.65340467e-12],\n",
      "       [ 2.10940498e-13,  0.00000000e+00,  0.00000000e+00,\n",
      "        -0.00000000e+00, -0.00000000e+00, -2.00962843e-13]]), 'dW5': array([[-2.26300128e-17,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [-6.87906072e-18,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00]]), 'db5': array([[-3.34054117e-13,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00, -1.01545615e-13]]), 'dX4': array([[-1.18691256e-17, -8.60038054e-17, -8.52049905e-17,\n",
      "        -1.03408416e-16,  1.32390930e-16],\n",
      "       [-1.18715807e-16,  2.89233965e-16,  5.04540182e-18,\n",
      "         4.44513716e-17, -1.41092834e-16],\n",
      "       [ 5.41995592e-17,  4.64751343e-17,  1.30841973e-16,\n",
      "         1.47249626e-16, -1.63160694e-16],\n",
      "       [ 1.33035907e-17, -5.05410553e-17, -1.40860253e-17,\n",
      "        -2.19950384e-17,  3.89211014e-17]]), 'dZ4': array([[-1.18691256e-17, -0.00000000e+00, -0.00000000e+00,\n",
      "        -0.00000000e+00,  0.00000000e+00],\n",
      "       [-1.18715807e-16,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00, -0.00000000e+00],\n",
      "       [ 5.41995592e-17,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00, -0.00000000e+00],\n",
      "       [ 1.33035907e-17, -0.00000000e+00, -0.00000000e+00,\n",
      "        -0.00000000e+00,  0.00000000e+00]]), 'dW4': array([[-5.27375243e-21, -4.57046988e-21,  0.00000000e+00,\n",
      "        -5.43658947e-22],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00]]), 'db4': array([[-1.57704458e-17,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00]]), 'dX3': array([[-3.35578079e-22, -5.43228060e-22,  1.41336161e-21,\n",
      "        -8.62248483e-22],\n",
      "       [-3.35647493e-21, -5.43340426e-21,  1.41365397e-20,\n",
      "        -8.62426838e-21],\n",
      "       [ 1.53239460e-21,  2.48061419e-21, -6.45402019e-21,\n",
      "         3.93739936e-21],\n",
      "       [ 3.76134986e-22,  6.08880888e-22, -1.58417603e-21,\n",
      "         9.66457111e-22]]), 'dZ3': array([[-3.35578079e-22, -5.43228060e-22,  0.00000000e+00,\n",
      "        -8.62248483e-22],\n",
      "       [-3.35647493e-21, -5.43340426e-21,  0.00000000e+00,\n",
      "        -8.62426838e-21],\n",
      "       [ 1.53239460e-21,  2.48061419e-21, -0.00000000e+00,\n",
      "         3.93739936e-21],\n",
      "       [ 3.76134986e-22,  6.08880888e-22, -0.00000000e+00,\n",
      "         9.66457111e-22]]), 'dW3': array([[ 0.00000000e+00,  0.00000000e+00, -1.10277770e-25],\n",
      "       [ 0.00000000e+00,  0.00000000e+00, -1.78515770e-25],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00, -2.83352358e-25]]), 'db3': array([[-4.45880857e-22, -7.21784312e-22,  0.00000000e+00,\n",
      "        -1.14566510e-21]]), 'dX2': array([[-1.77545946e-25, -1.08556890e-25, -1.05560736e-25],\n",
      "       [-1.77582671e-24, -1.08579344e-24, -1.05582571e-24],\n",
      "       [ 8.10751550e-25,  4.95717692e-25,  4.82035959e-25],\n",
      "       [ 1.99003588e-25,  1.21676732e-25,  1.18318473e-25]]), 'dZ2': array([[-0.00000000e+00, -0.00000000e+00, -1.05560736e-25],\n",
      "       [-0.00000000e+00, -0.00000000e+00, -1.05582571e-24],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  4.82035959e-25],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  1.18318473e-25]]), 'dW2': array([[ 0.00000000e+00,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00],\n",
      "       [-3.38627865e-29, -2.26029096e-28]]), 'db2': array([[ 0.00000000e+00,  0.00000000e+00, -1.40258003e-25]]), 'dX1': array([[-1.95998887e-30, -2.99011418e-29],\n",
      "       [-1.96039429e-29, -2.99073268e-28],\n",
      "       [ 8.95015659e-30,  1.36541541e-28],\n",
      "       [ 2.19686694e-30,  3.35148993e-29]]), 'dZ1': array([[-1.95998887e-30, -2.99011418e-29],\n",
      "       [-1.96039429e-29, -2.99073268e-28],\n",
      "       [ 8.95015659e-30,  1.36541541e-28],\n",
      "       [ 2.19686694e-30,  3.35148993e-29]]), 'dW1': array([[4.65302628e-29],\n",
      "       [7.09855044e-28]]), 'db1': array([[-2.60422707e-30, -3.97294923e-29]])}\n"
     ]
    }
   ],
   "source": [
    "def d_relu(x):\n",
    "    dydx = np.ones_like(x)\n",
    "    dydx[x < 0] = 0\n",
    "    return dydx\n",
    "\n",
    "def grad(predictions, labels, params, cache):\n",
    "    num_layers = int(len(params) / 2)\n",
    "    grads = {'dZ' + str(num_layers): predictions - labels}\n",
    "    for i in reversed(range(num_layers)):\n",
    "        grads['dW' + str(i+1)] = grads['dZ' + str(i+1)].T @ cache['X' + str(i)]\n",
    "        grads['db' + str(i+1)] = np.mean(grads['dZ' + str(i+1)], axis=0, keepdims=True)\n",
    "        if i==0:\n",
    "            break\n",
    "        grads['dX' + str(i)] = grads['dZ' + str(i+1)] @ params['W' + str(i+1)]\n",
    "        grads['dZ' + str(i)] = grads['dX' + str(i)] * d_relu(cache['Z' + str(i)])\n",
    "\n",
    "    return grads\n",
    "\n",
    "# Sanity check\n",
    "dummy_grad = grad(dummy_preds, dummy_labels, dummy_params, dummy_cache)\n",
    "print(dummy_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Gradient Descent Model Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations 1: training loss: 1.3862940641906007, testing loss: 1.3862985782026833\n",
      "Iterations 2: training loss: 1.3862937225999665, testing loss: 1.3862982106878292\n",
      "Iterations 3: training loss: 1.3862933718933386, testing loss: 1.3862978342405077\n",
      "Iterations 4: training loss: 1.3862930065247927, testing loss: 1.3862974427974473\n",
      "Iterations 5: training loss: 1.3862926203475752, testing loss: 1.3862970293073162\n",
      "Iterations 6: training loss: 1.386292208004719, testing loss: 1.3862965880779938\n",
      "Iterations 7: training loss: 1.3862917637537926, testing loss: 1.386296112907559\n",
      "Iterations 8: training loss: 1.386291280034981, testing loss: 1.386295594937031\n",
      "Iterations 9: training loss: 1.386290748755732, testing loss: 1.386295025590598\n",
      "Iterations 10: training loss: 1.386290161966965, testing loss: 1.3862943962732424\n",
      "Iterations 11: training loss: 1.3862895110731686, testing loss: 1.3862936973267106\n",
      "Iterations 12: training loss: 1.3862887861525053, testing loss: 1.3862929183184305\n",
      "Iterations 13: training loss: 1.3862879768035838, testing loss: 1.3862920475963345\n",
      "Iterations 14: training loss: 1.3862870704698456, testing loss: 1.386291071187226\n",
      "Iterations 15: training loss: 1.386286053570765, testing loss: 1.3862899742515582\n",
      "Iterations 16: training loss: 1.3862849100398507, testing loss: 1.386288739169657\n",
      "Iterations 17: training loss: 1.3862836234775957, testing loss: 1.3862873484520704\n",
      "Iterations 18: training loss: 1.3862821735983775, testing loss: 1.3862857791349694\n",
      "Iterations 19: training loss: 1.386280537743003, testing loss: 1.3862840071082212\n",
      "Iterations 20: training loss: 1.3862786902751403, testing loss: 1.3862820035746595\n",
      "Iterations 21: training loss: 1.3862766015594854, testing loss: 1.3862797365770814\n",
      "Iterations 22: training loss: 1.3862742401860533, testing loss: 1.3862771715102515\n",
      "Iterations 23: training loss: 1.386271568759572, testing loss: 1.3862742671989836\n",
      "Iterations 24: training loss: 1.3862685440721343, testing loss: 1.3862709772387518\n",
      "Iterations 25: training loss: 1.3862651179628762, testing loss: 1.3862672479133717\n",
      "Iterations 26: training loss: 1.3862612344086596, testing loss: 1.3862630185423233\n",
      "Iterations 27: training loss: 1.3862568326349054, testing loss: 1.3862582214223498\n",
      "Iterations 28: training loss: 1.3862518437342262, testing loss: 1.3862527811650103\n",
      "Iterations 29: training loss: 1.3862461870250524, testing loss: 1.386246608602586\n",
      "Iterations 30: training loss: 1.3862397712201038, testing loss: 1.3862396040317324\n",
      "Iterations 31: training loss: 1.3862324935928008, testing loss: 1.3862316551839726\n",
      "Iterations 32: training loss: 1.386224236159738, testing loss: 1.386222633217518\n",
      "Iterations 33: training loss: 1.3862148656478832, testing loss: 1.386212392497229\n",
      "Iterations 34: training loss: 1.3862042296175223, testing loss: 1.3862007647632875\n",
      "Iterations 35: training loss: 1.3861921555311418, testing loss: 1.3861875607022254\n",
      "Iterations 36: training loss: 1.3861784470732559, testing loss: 1.3861725655277257\n",
      "Iterations 37: training loss: 1.3861628823605883, testing loss: 1.38615553515315\n",
      "Iterations 38: training loss: 1.3861452090633546, testing loss: 1.386136192889176\n",
      "Iterations 39: training loss: 1.3861251417469558, testing loss: 1.3861142260812656\n",
      "Iterations 40: training loss: 1.3861023557251724, testing loss: 1.3860892778977556\n",
      "Iterations 41: training loss: 1.3860764824957434, testing loss: 1.3860609437126385\n",
      "Iterations 42: training loss: 1.3860471051771774, testing loss: 1.386028766782415\n",
      "Iterations 43: training loss: 1.3860137511558246, testing loss: 1.385992227866421\n",
      "Iterations 44: training loss: 1.3859758851724606, testing loss: 1.385950739398553\n",
      "Iterations 45: training loss: 1.3859329015397657, testing loss: 1.3859036369661366\n",
      "Iterations 46: training loss: 1.385884115881792, testing loss: 1.3858501691756475\n",
      "Iterations 47: training loss: 1.3858287555643103, testing loss: 1.3857894880513364\n",
      "Iterations 48: training loss: 1.3857659474143813, testing loss: 1.385720634863737\n",
      "Iterations 49: training loss: 1.3856947078982702, testing loss: 1.385642530596425\n",
      "Iterations 50: training loss: 1.3856139301806096, testing loss: 1.3855539599102742\n",
      "Iterations 51: training loss: 1.385522369414555, testing loss: 1.3854535556802645\n",
      "Iterations 52: training loss: 1.385418629133787, testing loss: 1.385339785651153\n",
      "Iterations 53: training loss: 1.385301146390503, testing loss: 1.3852109345663617\n",
      "Iterations 54: training loss: 1.3851681739552195, testing loss: 1.3850650839458312\n",
      "Iterations 55: training loss: 1.3850177653387472, testing loss: 1.384900097331671\n",
      "Iterations 56: training loss: 1.3848477583710597, testing loss: 1.3847136010146068\n",
      "Iterations 57: training loss: 1.3846557594717537, testing loss: 1.3845029670976505\n",
      "Iterations 58: training loss: 1.3844391305022856, testing loss: 1.384265299942363\n",
      "Iterations 59: training loss: 1.3841949768695987, testing loss: 1.383997421459238\n",
      "Iterations 60: training loss: 1.3839201408676718, testing loss: 1.383695865679229\n",
      "Iterations 61: training loss: 1.383611199641812, testing loss: 1.3833568749816483\n",
      "Iterations 62: training loss: 1.3832644725471162, testing loss: 1.3829764091342571\n",
      "Iterations 63: training loss: 1.3828760376626412, testing loss: 1.3825501633791193\n",
      "Iterations 64: training loss: 1.3824417627652013, testing loss: 1.3820736018091793\n",
      "Iterations 65: training loss: 1.3819573533849512, testing loss: 1.3815420116820833\n",
      "Iterations 66: training loss: 1.3814184217000787, testing loss: 1.3809505792175392\n",
      "Iterations 67: training loss: 1.38082057807792, testing loss: 1.3802944894629368\n",
      "Iterations 68: training loss: 1.3801595560594542, testing loss: 1.379569065436789\n",
      "Iterations 69: training loss: 1.379431363965401, testing loss: 1.3787699359323227\n",
      "Iterations 70: training loss: 1.3786324729694572, testing loss: 1.3778932427991204\n",
      "Iterations 71: training loss: 1.3777600385917625, testing loss: 1.3769358860937018\n",
      "Iterations 72: training loss: 1.376812153356131, testing loss: 1.3758958035287903\n",
      "Iterations 73: training loss: 1.3757881215516823, testing loss: 1.3747722768158934\n",
      "Iterations 74: training loss: 1.3746887480429142, testing loss: 1.3735662528818533\n",
      "Iterations 75: training loss: 1.3735166185063457, testing loss: 1.3722806567890145\n",
      "Iterations 76: training loss: 1.3722763480085314, testing loss: 1.3709206701333936\n",
      "Iterations 77: training loss: 1.3709747675117945, testing loss: 1.3694939438984604\n",
      "Iterations 78: training loss: 1.369621016944421, testing loss: 1.3680107070453538\n",
      "Iterations 79: training loss: 1.3682265064739558, testing loss: 1.3664837326193269\n",
      "Iterations 80: training loss: 1.3668047177310876, testing loss: 1.3649281256601498\n",
      "Iterations 81: training loss: 1.3653708298333074, testing loss: 1.3633609178336816\n",
      "Iterations 82: training loss: 1.3639411670560884, testing loss: 1.361800461828016\n",
      "Iterations 83: training loss: 1.3625324939586274, testing loss: 1.360265657214283\n",
      "Iterations 84: training loss: 1.3611612025792021, testing loss: 1.3587750496983395\n",
      "Iterations 85: training loss: 1.3598424663929412, testing loss: 1.3573458885165328\n",
      "Iterations 86: training loss: 1.358589443774829, testing loss: 1.355993234276524\n",
      "Iterations 87: training loss: 1.3574126180403718, testing loss: 1.3547292106664195\n",
      "Iterations 88: training loss: 1.3563193459385166, testing loss: 1.3535624831712334\n",
      "Iterations 89: training loss: 1.3553136621839055, testing loss: 1.352498018539768\n",
      "Iterations 90: training loss: 1.354396349180696, testing loss: 1.3515371359070514\n",
      "Iterations 91: training loss: 1.3535652454380813, testing loss: 1.3506778277150118\n",
      "Iterations 92: training loss: 1.352815736569266, testing loss: 1.3499152846398945\n",
      "Iterations 93: training loss: 1.3521413548145569, testing loss: 1.3492425507366048\n",
      "Iterations 94: training loss: 1.351534410375898, testing loss: 1.348651216863697\n",
      "Iterations 95: training loss: 1.3509865882200673, testing loss: 1.3481320861131971\n",
      "Iterations 96: training loss: 1.3504894602030226, testing loss: 1.3476757511343365\n",
      "Iterations 97: training loss: 1.350034886837862, testing loss: 1.347273054012279\n",
      "Iterations 98: training loss: 1.3496153003098152, testing loss: 1.346915415414848\n",
      "Iterations 99: training loss: 1.349223876132804, testing loss: 1.3465950428969864\n",
      "Iterations 100: training loss: 1.348854612553637, testing loss: 1.346305036525101\n",
      "Iterations 101: training loss: 1.3485023376466534, testing loss: 1.346039409769085\n",
      "Iterations 102: training loss: 1.348162667606607, testing loss: 1.3457930558200675\n",
      "Iterations 103: training loss: 1.347831932533134, testing loss: 1.3455616774099457\n",
      "Iterations 104: training loss: 1.347507088577326, testing loss: 1.3453416964503964\n",
      "Iterations 105: training loss: 1.347185625896391, testing loss: 1.3451301541973009\n",
      "Iterations 106: training loss: 1.3468654791961705, testing loss: 1.344924617210266\n",
      "Iterations 107: training loss: 1.3465449462267967, testing loss: 1.3447230863349247\n",
      "Iterations 108: training loss: 1.3462226149035565, testing loss: 1.3445239139536362\n",
      "Iterations 109: training loss: 1.3458972992119773, testing loss: 1.3443257441832681\n",
      "Iterations 110: training loss: 1.345568007195952, testing loss: 1.3441274533024716\n",
      "Iterations 111: training loss: 1.3452338797392773, testing loss: 1.3439280970092584\n",
      "Iterations 112: training loss: 1.3448941613930994, testing loss: 1.3437268698386884\n",
      "Iterations 113: training loss: 1.3445481747735593, testing loss: 1.3435230789008754\n",
      "Iterations 114: training loss: 1.3441952981446745, testing loss: 1.3433161091180488\n",
      "Iterations 115: training loss: 1.3438349504070586, testing loss: 1.3431054120797787\n",
      "Iterations 116: training loss: 1.343466578574205, testing loss: 1.3428904820955305\n",
      "Iterations 117: training loss: 1.3430896385143776, testing loss: 1.3426708484592431\n",
      "Iterations 118: training loss: 1.342703598636165, testing loss: 1.3424460635249356\n",
      "Iterations 119: training loss: 1.3423079300971712, testing loss: 1.342215688867113\n",
      "Iterations 120: training loss: 1.3419021004084648, testing loss: 1.3419792894440934\n",
      "Iterations 121: training loss: 1.341485561515646, testing loss: 1.3417364293152074\n",
      "Iterations 122: training loss: 1.3410577448599754, testing loss: 1.3414866741597935\n",
      "Iterations 123: training loss: 1.3406180719384666, testing loss: 1.3412295880519869\n",
      "Iterations 124: training loss: 1.340165946056646, testing loss: 1.3409647048793178\n",
      "Iterations 125: training loss: 1.3397007466014617, testing loss: 1.3406915569042452\n",
      "Iterations 126: training loss: 1.339221830025126, testing loss: 1.3404096493360378\n",
      "Iterations 127: training loss: 1.3387285378687956, testing loss: 1.340118482794351\n",
      "Iterations 128: training loss: 1.3382201979505308, testing loss: 1.3398175526703855\n",
      "Iterations 129: training loss: 1.3376960899642192, testing loss: 1.339506316844929\n",
      "Iterations 130: training loss: 1.3371554470272529, testing loss: 1.33918417996445\n",
      "Iterations 131: training loss: 1.3365975134147714, testing loss: 1.3388505600329208\n",
      "Iterations 132: training loss: 1.3360215371963593, testing loss: 1.3385049390077064\n",
      "Iterations 133: training loss: 1.3354267155723405, testing loss: 1.33814671762227\n",
      "Iterations 134: training loss: 1.3348121452995005, testing loss: 1.3377752332549353\n",
      "Iterations 135: training loss: 1.3341768256336586, testing loss: 1.337389681073797\n",
      "Iterations 136: training loss: 1.3335197468673985, testing loss: 1.3369892121518971\n",
      "Iterations 137: training loss: 1.3328398717440113, testing loss: 1.3365729994269708\n",
      "Iterations 138: training loss: 1.3321362864792767, testing loss: 1.3361403968575474\n",
      "Iterations 139: training loss: 1.3314084843576068, testing loss: 1.3356910841921914\n",
      "Iterations 140: training loss: 1.3306558947319405, testing loss: 1.3352251517462528\n",
      "Iterations 141: training loss: 1.329877267316385, testing loss: 1.3347414820224464\n",
      "Iterations 142: training loss: 1.3290711823647947, testing loss: 1.334238957127872\n",
      "Iterations 143: training loss: 1.3282362924172604, testing loss: 1.333716408764802\n",
      "Iterations 144: training loss: 1.3273715051707142, testing loss: 1.3331729473783418\n",
      "Iterations 145: training loss: 1.3264766079188985, testing loss: 1.3326088295006426\n",
      "Iterations 146: training loss: 1.3255513327228328, testing loss: 1.3320241268689788\n",
      "Iterations 147: training loss: 1.3245943953934574, testing loss: 1.3314181182460116\n",
      "Iterations 148: training loss: 1.323604283516797, testing loss: 1.3307894510549265\n",
      "Iterations 149: training loss: 1.3225803718011193, testing loss: 1.3301380660421107\n",
      "Iterations 150: training loss: 1.321522232334865, testing loss: 1.329463842339207\n",
      "Iterations 151: training loss: 1.3204287947479776, testing loss: 1.3287655592885157\n",
      "Iterations 152: training loss: 1.3192990237831965, testing loss: 1.3280433875136506\n",
      "Iterations 153: training loss: 1.3181327733615922, testing loss: 1.3272975676126073\n",
      "Iterations 154: training loss: 1.316930799212991, testing loss: 1.3265285518099748\n",
      "Iterations 155: training loss: 1.3156919257934516, testing loss: 1.3257352392642572\n",
      "Iterations 156: training loss: 1.314417247271421, testing loss: 1.3249214649351668\n",
      "Iterations 157: training loss: 1.313112627559985, testing loss: 1.324093165053754\n",
      "Iterations 158: training loss: 1.3117831433735685, testing loss: 1.323255217423012\n",
      "Iterations 159: training loss: 1.3104244794151605, testing loss: 1.3224024111976562\n",
      "Iterations 160: training loss: 1.3090325484538987, testing loss: 1.321531440049665\n",
      "Iterations 161: training loss: 1.3076041914072127, testing loss: 1.3206410257865444\n",
      "Iterations 162: training loss: 1.3061381405051629, testing loss: 1.319727666093519\n",
      "Iterations 163: training loss: 1.3046357322210231, testing loss: 1.3187923641683057\n",
      "Iterations 164: training loss: 1.3030979486300052, testing loss: 1.3178356026002769\n",
      "Iterations 165: training loss: 1.3015232938475747, testing loss: 1.3168545108039635\n",
      "Iterations 166: training loss: 1.2999116342026331, testing loss: 1.3158505448406657\n",
      "Iterations 167: training loss: 1.2982657207648394, testing loss: 1.3148244213439106\n",
      "Iterations 168: training loss: 1.2965928429725895, testing loss: 1.3137849166227886\n",
      "Iterations 169: training loss: 1.2948991020776266, testing loss: 1.3127374410092827\n",
      "Iterations 170: training loss: 1.2931975654186059, testing loss: 1.311692797940646\n",
      "Iterations 171: training loss: 1.2914888469050263, testing loss: 1.3106508057941864\n",
      "Iterations 172: training loss: 1.2897669724714487, testing loss: 1.3096103370262142\n",
      "Iterations 173: training loss: 1.2880250918612863, testing loss: 1.3085618334398965\n",
      "Iterations 174: training loss: 1.2862609890533894, testing loss: 1.3075016745444927\n",
      "Iterations 175: training loss: 1.2844737778374884, testing loss: 1.3064218111422112\n",
      "Iterations 176: training loss: 1.2826637656560114, testing loss: 1.3053214538884017\n",
      "Iterations 177: training loss: 1.2808294953049806, testing loss: 1.3041998158881172\n",
      "Iterations 178: training loss: 1.278971760726986, testing loss: 1.3030562304125133\n",
      "Iterations 179: training loss: 1.2770928324814264, testing loss: 1.3018942281296209\n",
      "Iterations 180: training loss: 1.2751958206731668, testing loss: 1.300715254001377\n",
      "Iterations 181: training loss: 1.2732816220408636, testing loss: 1.2995195152953027\n",
      "Iterations 182: training loss: 1.2713486752754037, testing loss: 1.2983032395075609\n",
      "Iterations 183: training loss: 1.269395020707315, testing loss: 1.297062751037726\n",
      "Iterations 184: training loss: 1.2674146732872251, testing loss: 1.2957890419381903\n",
      "Iterations 185: training loss: 1.2654042814072302, testing loss: 1.29448058555093\n",
      "Iterations 186: training loss: 1.263361294939164, testing loss: 1.2931346003950188\n",
      "Iterations 187: training loss: 1.261283667795686, testing loss: 1.291749137871565\n",
      "Iterations 188: training loss: 1.259169146000829, testing loss: 1.290318738643623\n",
      "Iterations 189: training loss: 1.2570176092229202, testing loss: 1.288848704942751\n",
      "Iterations 190: training loss: 1.2548281867790159, testing loss: 1.2873437987072915\n",
      "Iterations 191: training loss: 1.2525993596757476, testing loss: 1.285798436863737\n",
      "Iterations 192: training loss: 1.2503297162378129, testing loss: 1.2842068212300652\n",
      "Iterations 193: training loss: 1.2480185697610984, testing loss: 1.2825686468384279\n",
      "Iterations 194: training loss: 1.2456671101838641, testing loss: 1.280888375709848\n",
      "Iterations 195: training loss: 1.2432761931008842, testing loss: 1.2791726602449855\n",
      "Iterations 196: training loss: 1.2408471036066298, testing loss: 1.2774263265300716\n",
      "Iterations 197: training loss: 1.2383809752404213, testing loss: 1.275646674867099\n",
      "Iterations 198: training loss: 1.2358775012698393, testing loss: 1.2738304538312302\n",
      "Iterations 199: training loss: 1.2333384245355496, testing loss: 1.2719794881301663\n",
      "Iterations 200: training loss: 1.2307655241777746, testing loss: 1.2701037809536637\n",
      "Iterations 201: training loss: 1.2281599095737143, testing loss: 1.2682024163889818\n",
      "Iterations 202: training loss: 1.225523204693955, testing loss: 1.2662784501015376\n",
      "Iterations 203: training loss: 1.2228570564600212, testing loss: 1.2643296399625037\n",
      "Iterations 204: training loss: 1.2201635096429448, testing loss: 1.2623654753708222\n",
      "Iterations 205: training loss: 1.2174440794818207, testing loss: 1.2603852880957696\n",
      "Iterations 206: training loss: 1.2147005980502354, testing loss: 1.2583879084197447\n",
      "Iterations 207: training loss: 1.2119354350249638, testing loss: 1.256378176594939\n",
      "Iterations 208: training loss: 1.209151331684063, testing loss: 1.2543601896718666\n",
      "Iterations 209: training loss: 1.2063504113991537, testing loss: 1.2523340187234306\n",
      "Iterations 210: training loss: 1.2035350736703996, testing loss: 1.2503067172898608\n",
      "Iterations 211: training loss: 1.2007095204257585, testing loss: 1.248274566393251\n",
      "Iterations 212: training loss: 1.1978758274564207, testing loss: 1.2462446479176512\n",
      "Iterations 213: training loss: 1.1950375383295821, testing loss: 1.2442224388993322\n",
      "Iterations 214: training loss: 1.1922012971822902, testing loss: 1.2422054113480825\n",
      "Iterations 215: training loss: 1.1893713526406984, testing loss: 1.2402003917251987\n",
      "Iterations 216: training loss: 1.1865474756797625, testing loss: 1.2382070476914926\n",
      "Iterations 217: training loss: 1.18373045082924, testing loss: 1.2362288238676744\n",
      "Iterations 218: training loss: 1.180921465033811, testing loss: 1.2342630233731955\n",
      "Iterations 219: training loss: 1.1781220446087393, testing loss: 1.2323119686442183\n",
      "Iterations 220: training loss: 1.1753347328294155, testing loss: 1.230374136717478\n",
      "Iterations 221: training loss: 1.172559953106707, testing loss: 1.2284533077193447\n",
      "Iterations 222: training loss: 1.1697972800986387, testing loss: 1.2265554658366198\n",
      "Iterations 223: training loss: 1.1670482133899807, testing loss: 1.2246705308331995\n",
      "Iterations 224: training loss: 1.1643118326084487, testing loss: 1.2228001875824452\n",
      "Iterations 225: training loss: 1.1615901571106915, testing loss: 1.2209420965211104\n",
      "Iterations 226: training loss: 1.1588854324741389, testing loss: 1.2190999221613295\n",
      "Iterations 227: training loss: 1.156199661493572, testing loss: 1.2172719242859753\n",
      "Iterations 228: training loss: 1.1535337745549503, testing loss: 1.2154569314221162\n",
      "Iterations 229: training loss: 1.1508915355075457, testing loss: 1.2136609626960764\n",
      "Iterations 230: training loss: 1.1482761686105827, testing loss: 1.211878965710218\n",
      "Iterations 231: training loss: 1.145687095417429, testing loss: 1.2101121590894395\n",
      "Iterations 232: training loss: 1.1431240121891573, testing loss: 1.2083606679189012\n",
      "Iterations 233: training loss: 1.1405881268065465, testing loss: 1.2066181729081706\n",
      "Iterations 234: training loss: 1.1380798136167258, testing loss: 1.2048968362561248\n",
      "Iterations 235: training loss: 1.1356000515933133, testing loss: 1.2031900351390155\n",
      "Iterations 236: training loss: 1.1331471467783647, testing loss: 1.2014997298940056\n",
      "Iterations 237: training loss: 1.1307192310591692, testing loss: 1.1998278711284946\n",
      "Iterations 238: training loss: 1.1283147931640907, testing loss: 1.1981750029010152\n",
      "Iterations 239: training loss: 1.1259327427452903, testing loss: 1.1965392129910615\n",
      "Iterations 240: training loss: 1.1235721240989205, testing loss: 1.194915836215367\n",
      "Iterations 241: training loss: 1.121232258583752, testing loss: 1.1933059877443617\n",
      "Iterations 242: training loss: 1.1189127238731411, testing loss: 1.1917027267544933\n",
      "Iterations 243: training loss: 1.1166134306201132, testing loss: 1.1901089291234612\n",
      "Iterations 244: training loss: 1.1143343270947532, testing loss: 1.1885160592256951\n",
      "Iterations 245: training loss: 1.1120747412304506, testing loss: 1.1869287619532622\n",
      "Iterations 246: training loss: 1.1098332589576203, testing loss: 1.1853510746043194\n",
      "Iterations 247: training loss: 1.1076096759262428, testing loss: 1.1837718606834593\n",
      "Iterations 248: training loss: 1.1054021211276903, testing loss: 1.182200573468241\n",
      "Iterations 249: training loss: 1.1032097123206246, testing loss: 1.180632531364906\n",
      "Iterations 250: training loss: 1.1010332538271848, testing loss: 1.179045973649431\n",
      "Iterations 251: training loss: 1.098871377362446, testing loss: 1.1774631980409782\n",
      "Iterations 252: training loss: 1.0967242925142058, testing loss: 1.1758724937391103\n",
      "Iterations 253: training loss: 1.0945911168496987, testing loss: 1.1742861809987137\n",
      "Iterations 254: training loss: 1.0924708595828427, testing loss: 1.1726944761946434\n",
      "Iterations 255: training loss: 1.090362898573845, testing loss: 1.1710971007331643\n",
      "Iterations 256: training loss: 1.088266088110913, testing loss: 1.1694937171432653\n",
      "Iterations 257: training loss: 1.0861800009299187, testing loss: 1.167887314925744\n",
      "Iterations 258: training loss: 1.0841039227344622, testing loss: 1.1662767665658478\n",
      "Iterations 259: training loss: 1.0820367324449622, testing loss: 1.1646623562592389\n",
      "Iterations 260: training loss: 1.0799782286124935, testing loss: 1.1630392628312936\n",
      "Iterations 261: training loss: 1.0779266938710714, testing loss: 1.1614165042562559\n",
      "Iterations 262: training loss: 1.0758822950891842, testing loss: 1.1597796149153428\n",
      "Iterations 263: training loss: 1.073844653924813, testing loss: 1.1581346283663334\n",
      "Iterations 264: training loss: 1.0718134747062904, testing loss: 1.1564813066055668\n",
      "Iterations 265: training loss: 1.0697887083396433, testing loss: 1.1548092748001377\n",
      "Iterations 266: training loss: 1.0677693103262407, testing loss: 1.153135208905649\n",
      "Iterations 267: training loss: 1.0657541478366026, testing loss: 1.1514563113967946\n",
      "Iterations 268: training loss: 1.0637431555208208, testing loss: 1.1497639173020502\n",
      "Iterations 269: training loss: 1.0617363469841503, testing loss: 1.1480645642744098\n",
      "Iterations 270: training loss: 1.0597336235034984, testing loss: 1.1463510006427498\n",
      "Iterations 271: training loss: 1.0577338734719228, testing loss: 1.144626832246459\n",
      "Iterations 272: training loss: 1.0557361373141254, testing loss: 1.1428992607473951\n",
      "Iterations 273: training loss: 1.0537407410984625, testing loss: 1.1411515499066924\n",
      "Iterations 274: training loss: 1.0517474809413478, testing loss: 1.1393920089009775\n",
      "Iterations 275: training loss: 1.0497565933641364, testing loss: 1.1376234841794792\n",
      "Iterations 276: training loss: 1.0477674247652276, testing loss: 1.1358471639488676\n",
      "Iterations 277: training loss: 1.0457792826291807, testing loss: 1.1340524756741182\n",
      "Iterations 278: training loss: 1.0437934212378064, testing loss: 1.1322499239460846\n",
      "Iterations 279: training loss: 1.0418094926919916, testing loss: 1.1304285980487283\n",
      "Iterations 280: training loss: 1.039827363552585, testing loss: 1.1286037154333495\n",
      "Iterations 281: training loss: 1.0378470276217127, testing loss: 1.1267696103592755\n",
      "Iterations 282: training loss: 1.035867842051472, testing loss: 1.1249228666246613\n",
      "Iterations 283: training loss: 1.0338915296095554, testing loss: 1.1230699225571792\n",
      "Iterations 284: training loss: 1.031918513698971, testing loss: 1.1212119215662046\n",
      "Iterations 285: training loss: 1.029947374167816, testing loss: 1.1193481152081541\n",
      "Iterations 286: training loss: 1.0279791859802294, testing loss: 1.1174777357710275\n",
      "Iterations 287: training loss: 1.026013838721682, testing loss: 1.1156001713093757\n",
      "Iterations 288: training loss: 1.024051242843065, testing loss: 1.113718506858184\n",
      "Iterations 289: training loss: 1.022093353762684, testing loss: 1.1118257081341343\n",
      "Iterations 290: training loss: 1.0201395750620414, testing loss: 1.1099351340328412\n",
      "Iterations 291: training loss: 1.018190098028566, testing loss: 1.108035623936079\n",
      "Iterations 292: training loss: 1.0162430472188544, testing loss: 1.106142910395033\n",
      "Iterations 293: training loss: 1.0142974782818892, testing loss: 1.1042476842669062\n",
      "Iterations 294: training loss: 1.012355233867718, testing loss: 1.1023456608243685\n",
      "Iterations 295: training loss: 1.0104168722935465, testing loss: 1.1004373619031442\n",
      "Iterations 296: training loss: 1.0084813740291338, testing loss: 1.0985345701228484\n",
      "Iterations 297: training loss: 1.0065490931824654, testing loss: 1.0966261390819352\n",
      "Iterations 298: training loss: 1.004620017102604, testing loss: 1.0947285342111615\n",
      "Iterations 299: training loss: 1.0026942822904186, testing loss: 1.0928270336053447\n",
      "Iterations 300: training loss: 1.000771715062329, testing loss: 1.0909262666323414\n",
      "Iterations 301: training loss: 0.9988525969106212, testing loss: 1.0890256734973345\n",
      "Iterations 302: training loss: 0.9969372760964949, testing loss: 1.0871237973773225\n",
      "Iterations 303: training loss: 0.9950253821700639, testing loss: 1.0852271535981741\n",
      "Iterations 304: training loss: 0.993117276563763, testing loss: 1.083329357168887\n",
      "Iterations 305: training loss: 0.9912130595093743, testing loss: 1.0814358121917653\n",
      "Iterations 306: training loss: 0.9893123561555278, testing loss: 1.0795374987802975\n",
      "Iterations 307: training loss: 0.987415869015284, testing loss: 1.0776446204824883\n",
      "Iterations 308: training loss: 0.9855236567685639, testing loss: 1.075752739416695\n",
      "Iterations 309: training loss: 0.9836357940208572, testing loss: 1.0738612799813174\n",
      "Iterations 310: training loss: 0.9817524535192362, testing loss: 1.071970968176561\n",
      "Iterations 311: training loss: 0.9798736890624429, testing loss: 1.0700955502627922\n",
      "Iterations 312: training loss: 0.9779995581771579, testing loss: 1.068221298996078\n",
      "Iterations 313: training loss: 0.9761302522381363, testing loss: 1.0663488326059534\n",
      "Iterations 314: training loss: 0.9742661441871772, testing loss: 1.064473982582392\n",
      "Iterations 315: training loss: 0.9724070462516019, testing loss: 1.062605159222824\n",
      "Iterations 316: training loss: 0.9705534336313415, testing loss: 1.0607420795124232\n",
      "Iterations 317: training loss: 0.9687049520761389, testing loss: 1.0588799907109776\n",
      "Iterations 318: training loss: 0.9668622380801272, testing loss: 1.0570289026823758\n",
      "Iterations 319: training loss: 0.965025418301269, testing loss: 1.0551815052201343\n",
      "Iterations 320: training loss: 0.96319440973187, testing loss: 1.0533348846965187\n",
      "Iterations 321: training loss: 0.9613696215671812, testing loss: 1.0514971370710091\n",
      "Iterations 322: training loss: 0.9595511143282117, testing loss: 1.0496605906838208\n",
      "Iterations 323: training loss: 0.9577389566757968, testing loss: 1.0478272740576624\n",
      "Iterations 324: training loss: 0.9559331900674038, testing loss: 1.0460094797987782\n",
      "Iterations 325: training loss: 0.954133760168466, testing loss: 1.0442005862170465\n",
      "Iterations 326: training loss: 0.9523411950746868, testing loss: 1.0423911525464584\n",
      "Iterations 327: training loss: 0.9505550212653802, testing loss: 1.0405932796593158\n",
      "Iterations 328: training loss: 0.9487751432212272, testing loss: 1.0387908892951927\n",
      "Iterations 329: training loss: 0.94700255419838, testing loss: 1.0370036048729052\n",
      "Iterations 330: training loss: 0.9452370978913952, testing loss: 1.0352239364907894\n",
      "Iterations 331: training loss: 0.9434789968205244, testing loss: 1.0334461444729557\n",
      "Iterations 332: training loss: 0.9417286697186525, testing loss: 1.0316790084402696\n",
      "Iterations 333: training loss: 0.9399854806699138, testing loss: 1.0299186734307422\n",
      "Iterations 334: training loss: 0.9382500887693791, testing loss: 1.028168761484722\n",
      "Iterations 335: training loss: 0.9365218131708124, testing loss: 1.0264184732641009\n",
      "Iterations 336: training loss: 0.9348007488557294, testing loss: 1.0246828287622995\n",
      "Iterations 337: training loss: 0.9330878111211496, testing loss: 1.022952854448721\n",
      "Iterations 338: training loss: 0.9313827070745164, testing loss: 1.02123896106164\n",
      "Iterations 339: training loss: 0.929685617210216, testing loss: 1.0195293748144472\n",
      "Iterations 340: training loss: 0.9279956742507751, testing loss: 1.0178189988864945\n",
      "Iterations 341: training loss: 0.9263134390721582, testing loss: 1.0161163911438944\n",
      "Iterations 342: training loss: 0.9246396352901921, testing loss: 1.014435980425952\n",
      "Iterations 343: training loss: 0.9229742880465819, testing loss: 1.0127551759840718\n",
      "Iterations 344: training loss: 0.921317274089725, testing loss: 1.0110823212145323\n",
      "Iterations 345: training loss: 0.9196687248066665, testing loss: 1.0094195290082548\n",
      "Iterations 346: training loss: 0.9180284684548798, testing loss: 1.0077626467128185\n",
      "Iterations 347: training loss: 0.9163965950064192, testing loss: 1.0061205029106728\n",
      "Iterations 348: training loss: 0.9147731462944328, testing loss: 1.0044827165817485\n",
      "Iterations 349: training loss: 0.9131579804349645, testing loss: 1.0028623375006818\n",
      "Iterations 350: training loss: 0.9115507808333928, testing loss: 1.0012332506293864\n",
      "Iterations 351: training loss: 0.9099521499198674, testing loss: 0.999628068500062\n",
      "Iterations 352: training loss: 0.908362479515448, testing loss: 0.9980276992446705\n",
      "Iterations 353: training loss: 0.9067817100142073, testing loss: 0.9964280446748521\n",
      "Iterations 354: training loss: 0.9052096818012425, testing loss: 0.9948414085482936\n",
      "Iterations 355: training loss: 0.9036463055193619, testing loss: 0.9932690817376616\n",
      "Iterations 356: training loss: 0.9020914126326036, testing loss: 0.9916976824205196\n",
      "Iterations 357: training loss: 0.9005452877473735, testing loss: 0.9901431087187703\n",
      "Iterations 358: training loss: 0.899008068399328, testing loss: 0.9886004293632\n",
      "Iterations 359: training loss: 0.8974798871753052, testing loss: 0.9870627998814732\n",
      "Iterations 360: training loss: 0.8959599901585198, testing loss: 0.9855310290230794\n",
      "Iterations 361: training loss: 0.8944489086500048, testing loss: 0.9840169128656611\n",
      "Iterations 362: training loss: 0.8929465680668764, testing loss: 0.9825057961536223\n",
      "Iterations 363: training loss: 0.8914526682144988, testing loss: 0.9810041241522955\n",
      "Iterations 364: training loss: 0.8899672519536826, testing loss: 0.979513521392604\n",
      "Iterations 365: training loss: 0.8884900762432016, testing loss: 0.9780338242563983\n",
      "Iterations 366: training loss: 0.8870218163298079, testing loss: 0.9765587453947656\n",
      "Iterations 367: training loss: 0.885562846643911, testing loss: 0.9750943187267397\n",
      "Iterations 368: training loss: 0.8841129114516294, testing loss: 0.9736440782089593\n",
      "Iterations 369: training loss: 0.8826719394120677, testing loss: 0.9721967170692466\n",
      "Iterations 370: training loss: 0.8812400239250789, testing loss: 0.9707655587598814\n",
      "Iterations 371: training loss: 0.8798166827138231, testing loss: 0.9693399034945018\n",
      "Iterations 372: training loss: 0.8784023774252916, testing loss: 0.9679285286824949\n",
      "Iterations 373: training loss: 0.8769970827242028, testing loss: 0.9665247153921136\n",
      "Iterations 374: training loss: 0.8756006317953305, testing loss: 0.9651274733752242\n",
      "Iterations 375: training loss: 0.8742130828929324, testing loss: 0.9637399396567199\n",
      "Iterations 376: training loss: 0.8728342034604359, testing loss: 0.962357604073487\n",
      "Iterations 377: training loss: 0.8714643413431282, testing loss: 0.9609914962037222\n",
      "Iterations 378: training loss: 0.8701033646674722, testing loss: 0.9596298982246273\n",
      "Iterations 379: training loss: 0.8687507924786153, testing loss: 0.9582801190136818\n",
      "Iterations 380: training loss: 0.8674068699110842, testing loss: 0.9569343075146446\n",
      "Iterations 381: training loss: 0.8660716542248579, testing loss: 0.9556055223244804\n",
      "Iterations 382: training loss: 0.8647448017457273, testing loss: 0.9542796071311177\n",
      "Iterations 383: training loss: 0.8634265042398767, testing loss: 0.952965773628156\n",
      "Iterations 384: training loss: 0.862116563841987, testing loss: 0.9516606877027521\n",
      "Iterations 385: training loss: 0.8608152122852911, testing loss: 0.9503617438277729\n",
      "Iterations 386: training loss: 0.8595222430673494, testing loss: 0.949077349612367\n",
      "Iterations 387: training loss: 0.8582378067307048, testing loss: 0.9477948523464101\n",
      "Iterations 388: training loss: 0.8569616323092858, testing loss: 0.9465256187827226\n",
      "Iterations 389: training loss: 0.8556939073417666, testing loss: 0.9452653718379243\n",
      "Iterations 390: training loss: 0.8544344877490417, testing loss: 0.9440142020041558\n",
      "Iterations 391: training loss: 0.853182385100169, testing loss: 0.9427661643300023\n",
      "Iterations 392: training loss: 0.8519387089845624, testing loss: 0.9415305061652284\n",
      "Iterations 393: training loss: 0.8507032111583406, testing loss: 0.9403029400828469\n",
      "Iterations 394: training loss: 0.8494758749553583, testing loss: 0.9390849358520256\n",
      "Iterations 395: training loss: 0.848256709903135, testing loss: 0.9378764317073603\n",
      "Iterations 396: training loss: 0.8470456441487305, testing loss: 0.9366739185834339\n",
      "Iterations 397: training loss: 0.8458425412149105, testing loss: 0.9354844129388378\n",
      "Iterations 398: training loss: 0.8446472211575853, testing loss: 0.9342993099904692\n",
      "Iterations 399: training loss: 0.8434598940369451, testing loss: 0.9331277694670991\n",
      "Iterations 400: training loss: 0.8422805191431075, testing loss: 0.9319578640208949\n",
      "Iterations 401: training loss: 0.841109038160498, testing loss: 0.9307975203423139\n",
      "Iterations 402: training loss: 0.8399452979862757, testing loss: 0.9296481182825626\n",
      "Iterations 403: training loss: 0.8387891234840041, testing loss: 0.9285014509226703\n",
      "Iterations 404: training loss: 0.8376404708576151, testing loss: 0.9273668641008463\n",
      "Iterations 405: training loss: 0.8364993836927559, testing loss: 0.9262422864058351\n",
      "Iterations 406: training loss: 0.8353658394771486, testing loss: 0.925122133685581\n",
      "Iterations 407: training loss: 0.8342397355633164, testing loss: 0.9240118887972503\n",
      "Iterations 408: training loss: 0.8331210686941428, testing loss: 0.9229043282318357\n",
      "Iterations 409: training loss: 0.8320096779220519, testing loss: 0.9218096704234858\n",
      "Iterations 410: training loss: 0.8309056708288646, testing loss: 0.9207158780762237\n",
      "Iterations 411: training loss: 0.8298089759634848, testing loss: 0.9196400211654833\n",
      "Iterations 412: training loss: 0.8287195153507076, testing loss: 0.9185658600752379\n",
      "Iterations 413: training loss: 0.8276371399368596, testing loss: 0.9175021665697535\n",
      "Iterations 414: training loss: 0.8265617675977678, testing loss: 0.9164416884393723\n",
      "Iterations 415: training loss: 0.8254933796976057, testing loss: 0.9153908561245745\n",
      "Iterations 416: training loss: 0.8244319923431552, testing loss: 0.9143464873009743\n",
      "Iterations 417: training loss: 0.8233775556099417, testing loss: 0.9133096679229298\n",
      "Iterations 418: training loss: 0.8223297509373981, testing loss: 0.9122807497484332\n",
      "Iterations 419: training loss: 0.8212887876833798, testing loss: 0.9112577739323001\n",
      "Iterations 420: training loss: 0.8202543749193832, testing loss: 0.9102428918325587\n",
      "Iterations 421: training loss: 0.8192267628730229, testing loss: 0.9092381597544209\n",
      "Iterations 422: training loss: 0.8182057854661274, testing loss: 0.9082409624997879\n",
      "Iterations 423: training loss: 0.8171913767686695, testing loss: 0.9072431557542052\n",
      "Iterations 424: training loss: 0.8161834965904466, testing loss: 0.9062559612174129\n",
      "Iterations 425: training loss: 0.8151820197411396, testing loss: 0.9052758068150164\n",
      "Iterations 426: training loss: 0.8141868649867583, testing loss: 0.9043005287060748\n",
      "Iterations 427: training loss: 0.8131981749348413, testing loss: 0.9033369347649227\n",
      "Iterations 428: training loss: 0.8122158290153626, testing loss: 0.9023739843537883\n",
      "Iterations 429: training loss: 0.8112397272826463, testing loss: 0.9014222685482007\n",
      "Iterations 430: training loss: 0.8102696999474024, testing loss: 0.9004726036066755\n",
      "Iterations 431: training loss: 0.8093057867643024, testing loss: 0.8995327919264303\n",
      "Iterations 432: training loss: 0.8083478227435749, testing loss: 0.898596464208336\n",
      "Iterations 433: training loss: 0.8073958341159732, testing loss: 0.897667771668552\n",
      "Iterations 434: training loss: 0.8064497748474224, testing loss: 0.8967467797134049\n",
      "Iterations 435: training loss: 0.8055094490919436, testing loss: 0.8958244370822253\n",
      "Iterations 436: training loss: 0.8045749182123562, testing loss: 0.8949138615999633\n",
      "Iterations 437: training loss: 0.8036460857503079, testing loss: 0.8940038620040789\n",
      "Iterations 438: training loss: 0.8027229045468743, testing loss: 0.8931043418991604\n",
      "Iterations 439: training loss: 0.8018054241207339, testing loss: 0.8922083112303327\n",
      "Iterations 440: training loss: 0.8008936674333996, testing loss: 0.8913208864080281\n",
      "Iterations 441: training loss: 0.7999875902586946, testing loss: 0.8904336048144375\n",
      "Iterations 442: training loss: 0.7990870541590869, testing loss: 0.8895582738519188\n",
      "Iterations 443: training loss: 0.7981919840937897, testing loss: 0.8886835432916718\n",
      "Iterations 444: training loss: 0.7973023599847113, testing loss: 0.8878184456390558\n",
      "Iterations 445: training loss: 0.7964181037185402, testing loss: 0.8869549711468617\n",
      "Iterations 446: training loss: 0.7955389450664344, testing loss: 0.8861005993960228\n",
      "Iterations 447: training loss: 0.7946650461574368, testing loss: 0.885247646994748\n",
      "Iterations 448: training loss: 0.7937963403397864, testing loss: 0.8844023402920091\n",
      "Iterations 449: training loss: 0.7929327066984023, testing loss: 0.8835598808038861\n",
      "Iterations 450: training loss: 0.7920740694419252, testing loss: 0.8827275409057225\n",
      "Iterations 451: training loss: 0.7912204162063701, testing loss: 0.8818918213102673\n",
      "Iterations 452: training loss: 0.7903716777152336, testing loss: 0.8810711098741676\n",
      "Iterations 453: training loss: 0.7895277604947978, testing loss: 0.8802463805224522\n",
      "Iterations 454: training loss: 0.7886889455918964, testing loss: 0.8794277257117186\n",
      "Iterations 455: training loss: 0.7878550280106157, testing loss: 0.8786103762535856\n",
      "Iterations 456: training loss: 0.7870259854459519, testing loss: 0.8778092693679146\n",
      "Iterations 457: training loss: 0.7862016995863762, testing loss: 0.8770016030695144\n",
      "Iterations 458: training loss: 0.7853821636256589, testing loss: 0.8762069650036813\n",
      "Iterations 459: training loss: 0.7845673119058825, testing loss: 0.8754162623686136\n",
      "Iterations 460: training loss: 0.783757135950294, testing loss: 0.8746374225417874\n",
      "Iterations 461: training loss: 0.7829514035228947, testing loss: 0.8738457286965019\n",
      "Iterations 462: training loss: 0.7821502097057372, testing loss: 0.8730769590994817\n",
      "Iterations 463: training loss: 0.7813536447569422, testing loss: 0.8722997542913195\n",
      "Iterations 464: training loss: 0.7805615689441344, testing loss: 0.8715402323719904\n",
      "Iterations 465: training loss: 0.7797738582551604, testing loss: 0.8707736570517269\n",
      "Iterations 466: training loss: 0.7789906054448703, testing loss: 0.8700230366720078\n",
      "Iterations 467: training loss: 0.7782116620066359, testing loss: 0.8692648678099976\n",
      "Iterations 468: training loss: 0.7774370248717211, testing loss: 0.868522482976452\n",
      "Iterations 469: training loss: 0.7766664770282664, testing loss: 0.8677677753188853\n",
      "Iterations 470: training loss: 0.775900138982392, testing loss: 0.8670408833581322\n",
      "Iterations 471: training loss: 0.7751379848559549, testing loss: 0.866295989966052\n",
      "Iterations 472: training loss: 0.7743797416365935, testing loss: 0.8655704558724796\n",
      "Iterations 473: training loss: 0.773625593650775, testing loss: 0.8648366701643022\n",
      "Iterations 474: training loss: 0.7728754873999243, testing loss: 0.8641191897101089\n",
      "Iterations 475: training loss: 0.7721293995949214, testing loss: 0.8633889896661583\n",
      "Iterations 476: training loss: 0.7713870741493222, testing loss: 0.8626811497523237\n",
      "Iterations 477: training loss: 0.7706486163529466, testing loss: 0.8619593622703287\n",
      "Iterations 478: training loss: 0.7699139567091151, testing loss: 0.861261861994583\n",
      "Iterations 479: training loss: 0.7691831228686382, testing loss: 0.8605453998096708\n",
      "Iterations 480: training loss: 0.7684560899673086, testing loss: 0.8598528601248313\n",
      "Iterations 481: training loss: 0.7677327969507088, testing loss: 0.8591408478305484\n",
      "Iterations 482: training loss: 0.767013263784791, testing loss: 0.8584620686903904\n",
      "Iterations 483: training loss: 0.7662974759933695, testing loss: 0.8577579900081734\n",
      "Iterations 484: training loss: 0.7655853604897033, testing loss: 0.8570862656106217\n",
      "Iterations 485: training loss: 0.7648768712163125, testing loss: 0.8563830894926413\n",
      "Iterations 486: training loss: 0.764171934870999, testing loss: 0.8557222767388486\n",
      "Iterations 487: training loss: 0.763470565385374, testing loss: 0.855024595239276\n",
      "Iterations 488: training loss: 0.7627727185838368, testing loss: 0.854371270709444\n",
      "Iterations 489: training loss: 0.7620782523483263, testing loss: 0.8536791270416516\n",
      "Iterations 490: training loss: 0.7613871800050975, testing loss: 0.8530352244153749\n",
      "Iterations 491: training loss: 0.7606994909455965, testing loss: 0.8523487604711982\n",
      "Iterations 492: training loss: 0.7600151087241296, testing loss: 0.8517166784736533\n",
      "Iterations 493: training loss: 0.7593339094940061, testing loss: 0.8510340243891785\n",
      "Iterations 494: training loss: 0.7586560657914865, testing loss: 0.8504096142152915\n",
      "Iterations 495: training loss: 0.7579814289654039, testing loss: 0.8497299902195335\n",
      "Iterations 496: training loss: 0.7573099536927012, testing loss: 0.8491087454500389\n",
      "Iterations 497: training loss: 0.756641709337742, testing loss: 0.8484341069631608\n",
      "Iterations 498: training loss: 0.7559766636633001, testing loss: 0.8478298994064077\n",
      "Iterations 499: training loss: 0.755314759354606, testing loss: 0.8471529775015496\n",
      "Iterations 500: training loss: 0.7546559020325165, testing loss: 0.8465591675047297\n",
      "Iterations 501: training loss: 0.7540001644329652, testing loss: 0.8458821261552056\n",
      "Iterations 502: training loss: 0.7533474623541828, testing loss: 0.8453007812782297\n",
      "Iterations 503: training loss: 0.7526978182295979, testing loss: 0.844613714831233\n",
      "Iterations 504: training loss: 0.7520511491350772, testing loss: 0.8440571809221407\n",
      "Iterations 505: training loss: 0.7514073762764764, testing loss: 0.843358993603878\n",
      "Iterations 506: training loss: 0.7507665913086263, testing loss: 0.8428283002865311\n",
      "Iterations 507: training loss: 0.7501287336834369, testing loss: 0.8421221158165395\n",
      "Iterations 508: training loss: 0.7494937632486598, testing loss: 0.8416075659566791\n",
      "Iterations 509: training loss: 0.7488617594447448, testing loss: 0.8408911639556222\n",
      "Iterations 510: training loss: 0.7482327333379569, testing loss: 0.8403996440936913\n",
      "Iterations 511: training loss: 0.7476066366711693, testing loss: 0.8396665253603293\n",
      "Iterations 512: training loss: 0.7469837067206951, testing loss: 0.8392066918931101\n",
      "Iterations 513: training loss: 0.7463636146202604, testing loss: 0.8384524166973342\n",
      "Iterations 514: training loss: 0.7457464321498707, testing loss: 0.8380339344017623\n",
      "Iterations 515: training loss: 0.745132176855902, testing loss: 0.8372420638669368\n",
      "Iterations 516: training loss: 0.7445207574788637, testing loss: 0.8368778270053062\n",
      "Iterations 517: training loss: 0.7439121877509302, testing loss: 0.8360386788435755\n",
      "Iterations 518: training loss: 0.7433070252246231, testing loss: 0.8357398124977273\n",
      "Iterations 519: training loss: 0.7427047732977283, testing loss: 0.8348431973223412\n",
      "Iterations 520: training loss: 0.7421061737497127, testing loss: 0.834627451264789\n",
      "Iterations 521: training loss: 0.7415104260985627, testing loss: 0.8336446253273684\n",
      "Iterations 522: training loss: 0.7409183260435148, testing loss: 0.8335323696023733\n",
      "Iterations 523: training loss: 0.7403300267874361, testing loss: 0.8324628492314998\n",
      "Iterations 524: training loss: 0.7397458850616644, testing loss: 0.8324609101019153\n",
      "Iterations 525: training loss: 0.7391659722304432, testing loss: 0.8312862284859155\n",
      "Iterations 526: training loss: 0.7385915440973643, testing loss: 0.8314228019551999\n",
      "Iterations 527: training loss: 0.7380223037450826, testing loss: 0.8301097755773276\n",
      "Iterations 528: training loss: 0.737461658907863, testing loss: 0.8304348965248002\n",
      "Iterations 529: training loss: 0.7369078941301058, testing loss: 0.8289336803052003\n",
      "Iterations 530: training loss: 0.736365234202427, testing loss: 0.8295126070312208\n",
      "Iterations 531: training loss: 0.7358340202935749, testing loss: 0.8277719385589161\n",
      "Iterations 532: training loss: 0.7353175792452854, testing loss: 0.8286781340459521\n",
      "Iterations 533: training loss: 0.7348205020095029, testing loss: 0.8266322702790793\n",
      "Iterations 534: training loss: 0.7343422643779393, testing loss: 0.82796052547629\n",
      "Iterations 535: training loss: 0.7338955337287655, testing loss: 0.8255362027750887\n",
      "Iterations 536: training loss: 0.7334815604534851, testing loss: 0.827457031677956\n",
      "Iterations 537: training loss: 0.7331067088925649, testing loss: 0.8245129766741148\n",
      "Iterations 538: training loss: 0.7327833295671031, testing loss: 0.8272214512365303\n",
      "Iterations 539: training loss: 0.732528348546703, testing loss: 0.8236429990810318\n",
      "Iterations 540: training loss: 0.7324036410644094, testing loss: 0.8274493559637557\n",
      "Iterations 541: training loss: 0.7323676715811097, testing loss: 0.823118020960267\n",
      "Iterations 542: training loss: 0.7325548437046788, testing loss: 0.8284388957990554\n",
      "Iterations 543: training loss: 0.7328913414089415, testing loss: 0.8231770535517091\n",
      "Iterations 544: training loss: 0.7336570784638569, testing loss: 0.8307260142932078\n",
      "Iterations 545: training loss: 0.7346154625281696, testing loss: 0.8243717899123368\n",
      "Iterations 546: training loss: 0.7363601869229052, testing loss: 0.8350781020301069\n",
      "Iterations 547: training loss: 0.7383834580616054, testing loss: 0.8276306411850188\n",
      "Iterations 548: training loss: 0.74187345789428, testing loss: 0.8430035827612284\n",
      "Iterations 549: training loss: 0.7457212964694132, testing loss: 0.8345672963884325\n",
      "Iterations 550: training loss: 0.7521576041978565, testing loss: 0.8567981002354246\n",
      "Iterations 551: training loss: 0.7589735155171058, testing loss: 0.8477029008006843\n",
      "Iterations 552: training loss: 0.7694397630201496, testing loss: 0.8789153169578806\n",
      "Iterations 553: training loss: 0.7803943374916328, testing loss: 0.8696265477480936\n",
      "Iterations 554: training loss: 0.7944235526816819, testing loss: 0.9098477862818777\n",
      "Iterations 555: training loss: 0.8105522762360717, testing loss: 0.9012506195399479\n",
      "Iterations 556: training loss: 0.8250582209559253, testing loss: 0.9467166757141997\n",
      "Iterations 557: training loss: 0.8458617957788492, testing loss: 0.9389172266383481\n",
      "Iterations 558: training loss: 0.8542888594463709, testing loss: 0.981175591697131\n",
      "Iterations 559: training loss: 0.8761784308847878, testing loss: 0.9716655121888487\n",
      "Iterations 560: training loss: 0.8701620925583385, testing loss: 0.9998990386403744\n",
      "Iterations 561: training loss: 0.8875877723067404, testing loss: 0.9842531535394479\n",
      "Iterations 562: training loss: 0.868468409916995, testing loss: 0.9983644620593844\n",
      "Iterations 563: training loss: 0.8781991634229408, testing loss: 0.9745904049152595\n",
      "Iterations 564: training loss: 0.8549326194192917, testing loss: 0.9831229846253194\n",
      "Iterations 565: training loss: 0.8572560275548231, testing loss: 0.9525005380283478\n",
      "Iterations 566: training loss: 0.8365593894917971, testing loss: 0.9620733767160343\n",
      "Iterations 567: training loss: 0.8336572106265205, testing loss: 0.9275110217105581\n",
      "Iterations 568: training loss: 0.8166186719376847, testing loss: 0.938958568939431\n",
      "Iterations 569: training loss: 0.8107835618289422, testing loss: 0.9032611812196254\n",
      "Iterations 570: training loss: 0.7975671890484415, testing loss: 0.9165693744293985\n",
      "Iterations 571: training loss: 0.7909121689883687, testing loss: 0.8822487703627845\n",
      "Iterations 572: training loss: 0.780913199417305, testing loss: 0.8967724264461675\n",
      "Iterations 573: training loss: 0.7743130530177715, testing loss: 0.8647514238943536\n",
      "Iterations 574: training loss: 0.7667633472107127, testing loss: 0.879725009180805\n",
      "Iterations 575: training loss: 0.7608270431497991, testing loss: 0.8506666148777804\n",
      "Iterations 576: training loss: 0.7547842648376254, testing loss: 0.8650428819019942\n",
      "Iterations 577: training loss: 0.7497453741138087, testing loss: 0.8392579093737907\n",
      "Iterations 578: training loss: 0.7451742961301271, testing loss: 0.8531140440904802\n",
      "Iterations 579: training loss: 0.7411040513517395, testing loss: 0.8304295456946208\n",
      "Iterations 580: training loss: 0.7375069288915407, testing loss: 0.8434879368351602\n",
      "Iterations 581: training loss: 0.7343005860927794, testing loss: 0.8235489007860084\n",
      "Iterations 582: training loss: 0.7316548711555136, testing loss: 0.8360649348149422\n",
      "Iterations 583: training loss: 0.7291074238641599, testing loss: 0.8183488341716194\n",
      "Iterations 584: training loss: 0.72709948006038, testing loss: 0.8302397962109185\n",
      "Iterations 585: training loss: 0.7250198026432055, testing loss: 0.8143026551400504\n",
      "Iterations 586: training loss: 0.7233937764340025, testing loss: 0.8254790182591227\n",
      "Iterations 587: training loss: 0.7216956346334567, testing loss: 0.8110667865419904\n",
      "Iterations 588: training loss: 0.720503714381328, testing loss: 0.8217857020948299\n",
      "Iterations 589: training loss: 0.7190889856209532, testing loss: 0.8085475684644462\n",
      "Iterations 590: training loss: 0.7181077550508814, testing loss: 0.8187545028378\n",
      "Iterations 591: training loss: 0.7168969779353429, testing loss: 0.8064257607134355\n",
      "Iterations 592: training loss: 0.7160957247594547, testing loss: 0.816268913419654\n",
      "Iterations 593: training loss: 0.7150375120786157, testing loss: 0.8045834641268466\n",
      "Iterations 594: training loss: 0.7143945446110511, testing loss: 0.8142303021115606\n",
      "Iterations 595: training loss: 0.7134654597474308, testing loss: 0.803024137850893\n",
      "Iterations 596: training loss: 0.712940748047933, testing loss: 0.8125233671275406\n",
      "Iterations 597: training loss: 0.7120999519191947, testing loss: 0.8016692235096128\n",
      "Iterations 598: training loss: 0.7116397592782618, testing loss: 0.8110501638599127\n",
      "Iterations 599: training loss: 0.7108831822367657, testing loss: 0.8004441369413174\n",
      "Iterations 600: training loss: 0.7104835093345456, testing loss: 0.8097840581612741\n",
      "Iterations 601: training loss: 0.709791447993425, testing loss: 0.7993346006733218\n",
      "Iterations 602: training loss: 0.7094510342759167, testing loss: 0.808701552447444\n",
      "Iterations 603: training loss: 0.7087972888665539, testing loss: 0.7983072128484938\n",
      "Iterations 604: training loss: 0.7085055463636453, testing loss: 0.8077521775766043\n",
      "Iterations 605: training loss: 0.7078984226217816, testing loss: 0.7973703985003867\n",
      "Iterations 606: training loss: 0.7076630039881724, testing loss: 0.8069566536004491\n",
      "Iterations 607: training loss: 0.7071078099921847, testing loss: 0.7965284841969936\n",
      "Iterations 608: training loss: 0.706922975862672, testing loss: 0.8063234549246223\n",
      "Iterations 609: training loss: 0.7064218711658687, testing loss: 0.795767558648806\n",
      "Iterations 610: training loss: 0.7063063934039905, testing loss: 0.8058803480850926\n",
      "Iterations 611: training loss: 0.7058574895972302, testing loss: 0.7951128431970822\n",
      "Iterations 612: training loss: 0.7058250393190056, testing loss: 0.8056350535271208\n",
      "Iterations 613: training loss: 0.7054221405228883, testing loss: 0.7945772822500131\n",
      "Iterations 614: training loss: 0.705485056464213, testing loss: 0.8056047164171536\n",
      "Iterations 615: training loss: 0.7051391382469729, testing loss: 0.794181764550744\n",
      "Iterations 616: training loss: 0.7053019650110154, testing loss: 0.8057934905951322\n",
      "Iterations 617: training loss: 0.7050123145980784, testing loss: 0.793930870240279\n",
      "Iterations 618: training loss: 0.7053022335207733, testing loss: 0.8062477618121053\n",
      "Iterations 619: training loss: 0.7050766565878428, testing loss: 0.7938568192861049\n",
      "Iterations 620: training loss: 0.7055331307894416, testing loss: 0.8070311607406503\n",
      "Iterations 621: training loss: 0.7053394604381318, testing loss: 0.7939750333976292\n",
      "Iterations 622: training loss: 0.7059849407748889, testing loss: 0.8081196914453324\n",
      "Iterations 623: training loss: 0.7058919248365629, testing loss: 0.7943735214572967\n",
      "Iterations 624: training loss: 0.7067649517935419, testing loss: 0.8096528297396577\n",
      "Iterations 625: training loss: 0.7067336282611922, testing loss: 0.7950766073562995\n",
      "Iterations 626: training loss: 0.7078419411740347, testing loss: 0.8115767432309711\n",
      "Iterations 627: training loss: 0.7079093835491562, testing loss: 0.7961100607381794\n",
      "Iterations 628: training loss: 0.7093448169356568, testing loss: 0.8140733273958664\n",
      "Iterations 629: training loss: 0.7095560168469383, testing loss: 0.79762243226787\n",
      "Iterations 630: training loss: 0.7112698706201122, testing loss: 0.8171152563321935\n",
      "Iterations 631: training loss: 0.7116098444745578, testing loss: 0.7995684865945323\n",
      "Iterations 632: training loss: 0.7136902463976592, testing loss: 0.8207708702784234\n",
      "Iterations 633: training loss: 0.7141432517568506, testing loss: 0.8020305526374629\n",
      "Iterations 634: training loss: 0.7164855936620926, testing loss: 0.8248905530681229\n",
      "Iterations 635: training loss: 0.716970078059758, testing loss: 0.8048246312426298\n",
      "Iterations 636: training loss: 0.7195828895175336, testing loss: 0.829361459054031\n",
      "Iterations 637: training loss: 0.7201257074899401, testing loss: 0.808007986704101\n",
      "Iterations 638: training loss: 0.7230394549286703, testing loss: 0.8342562747858525\n",
      "Iterations 639: training loss: 0.723543563921816, testing loss: 0.8115058041812623\n",
      "Iterations 640: training loss: 0.726811834772471, testing loss: 0.8394814621237036\n",
      "Iterations 641: training loss: 0.7271773587572175, testing loss: 0.8152813650752138\n",
      "Iterations 642: training loss: 0.7300640396236088, testing loss: 0.844032393450926\n",
      "Iterations 643: training loss: 0.7302058645635785, testing loss: 0.8184904568009878\n",
      "Iterations 644: training loss: 0.732782848408527, testing loss: 0.8478479995535733\n",
      "Iterations 645: training loss: 0.7326144257242556, testing loss: 0.8210900154528377\n",
      "Iterations 646: training loss: 0.7348599733585245, testing loss: 0.8508018790430031\n",
      "Iterations 647: training loss: 0.7342930593041145, testing loss: 0.8229757970280865\n",
      "Iterations 648: training loss: 0.7362389841755768, testing loss: 0.8528530401619825\n",
      "Iterations 649: training loss: 0.7352061335043836, testing loss: 0.8240537171135439\n",
      "Iterations 650: training loss: 0.7366352437527509, testing loss: 0.8536567113214516\n",
      "Iterations 651: training loss: 0.7350934015286532, testing loss: 0.8240569034637945\n",
      "Iterations 652: training loss: 0.7359707911896415, testing loss: 0.8531254856336273\n",
      "Iterations 653: training loss: 0.73388478200896, testing loss: 0.8228920746114131\n",
      "Iterations 654: training loss: 0.734308283657802, testing loss: 0.851322152153051\n",
      "Iterations 655: training loss: 0.7317587968797934, testing loss: 0.8207547504768289\n",
      "Iterations 656: training loss: 0.7316714295018536, testing loss: 0.8482828873428265\n",
      "Iterations 657: training loss: 0.728697210338997, testing loss: 0.8176335345628793\n",
      "Iterations 658: training loss: 0.7282177084953247, testing loss: 0.8441593685325874\n",
      "Iterations 659: training loss: 0.724950640923911, testing loss: 0.8138000420186923\n",
      "Iterations 660: training loss: 0.7241189250282618, testing loss: 0.8391902385710696\n",
      "Iterations 661: training loss: 0.7207369070058891, testing loss: 0.8094575206090121\n",
      "Iterations 662: training loss: 0.719668798570796, testing loss: 0.8337420076254155\n",
      "Iterations 663: training loss: 0.7161963196109863, testing loss: 0.8047794553784672\n",
      "Iterations 664: training loss: 0.7150544165760855, testing loss: 0.828012488555462\n",
      "Iterations 665: training loss: 0.7117209775304302, testing loss: 0.8001906589237388\n",
      "Iterations 666: training loss: 0.7105265638923406, testing loss: 0.8223322563896891\n",
      "Iterations 667: training loss: 0.707349527126905, testing loss: 0.7957380056701285\n",
      "Iterations 668: training loss: 0.7061269348597445, testing loss: 0.8167584332852156\n",
      "Iterations 669: training loss: 0.7032504937805883, testing loss: 0.7915815676081043\n",
      "Iterations 670: training loss: 0.702234907063493, testing loss: 0.8118062359753665\n",
      "Iterations 671: training loss: 0.6995976407627669, testing loss: 0.7878814795609809\n",
      "Iterations 672: training loss: 0.6985512539995676, testing loss: 0.8070710509644169\n",
      "Iterations 673: training loss: 0.696234759137278, testing loss: 0.7845099840066739\n",
      "Iterations 674: training loss: 0.6954641233752217, testing loss: 0.8030879120175821\n",
      "Iterations 675: training loss: 0.6933692253170199, testing loss: 0.7816574144993818\n",
      "Iterations 676: training loss: 0.6926675252432835, testing loss: 0.7994846016322562\n",
      "Iterations 677: training loss: 0.6907908586452448, testing loss: 0.7790952380059821\n",
      "Iterations 678: training loss: 0.6901438538253598, testing loss: 0.7962586358186546\n",
      "Iterations 679: training loss: 0.6884371345718693, testing loss: 0.7767742958104448\n",
      "Iterations 680: training loss: 0.6878801732129847, testing loss: 0.7933756603553572\n",
      "Iterations 681: training loss: 0.6863353416456924, testing loss: 0.7747023697281382\n",
      "Iterations 682: training loss: 0.6859782779897111, testing loss: 0.7909977192840897\n",
      "Iterations 683: training loss: 0.6845640453896426, testing loss: 0.7729435950876398\n",
      "Iterations 684: training loss: 0.6842947744301529, testing loss: 0.7889312028072036\n",
      "Iterations 685: training loss: 0.6830072934075965, testing loss: 0.7713853608077899\n",
      "Iterations 686: training loss: 0.6827999240240215, testing loss: 0.787120430031067\n",
      "Iterations 687: training loss: 0.6816099512749892, testing loss: 0.7699979097760088\n",
      "Iterations 688: training loss: 0.6814721104538007, testing loss: 0.7855344340734443\n",
      "Iterations 689: training loss: 0.680375117159065, testing loss: 0.7687680845550681\n",
      "Iterations 690: training loss: 0.6803049794267277, testing loss: 0.7842016896365683\n",
      "Iterations 691: training loss: 0.6792966430477809, testing loss: 0.7676693101463401\n",
      "Iterations 692: training loss: 0.6792781862966827, testing loss: 0.7830644304915687\n",
      "Iterations 693: training loss: 0.6783562014580194, testing loss: 0.7667223759166397\n",
      "Iterations 694: training loss: 0.6783782943746709, testing loss: 0.782101377872712\n",
      "Iterations 695: training loss: 0.6775237488700383, testing loss: 0.7658660802792217\n",
      "Iterations 696: training loss: 0.6775975847313728, testing loss: 0.781330521491775\n",
      "Iterations 697: training loss: 0.6768013508946633, testing loss: 0.7651040841890311\n",
      "Iterations 698: training loss: 0.6769440070719853, testing loss: 0.7807548697635324\n",
      "Iterations 699: training loss: 0.676199049914609, testing loss: 0.7644423040337304\n",
      "Iterations 700: training loss: 0.676435932485454, testing loss: 0.7804053728093641\n",
      "Iterations 701: training loss: 0.6757412850383675, testing loss: 0.7639089321576973\n",
      "Iterations 702: training loss: 0.6760967031156399, testing loss: 0.7803081018633388\n",
      "Iterations 703: training loss: 0.6754560090418463, testing loss: 0.7635390068827018\n",
      "Iterations 704: training loss: 0.6759343391936229, testing loss: 0.7804764701210244\n",
      "Iterations 705: training loss: 0.6753325913703846, testing loss: 0.763323111197698\n",
      "Iterations 706: training loss: 0.6759438976551225, testing loss: 0.7808982846523127\n",
      "Iterations 707: training loss: 0.6753625341143945, testing loss: 0.7632441952831786\n",
      "Iterations 708: training loss: 0.6761443467845526, testing loss: 0.7815918790206358\n",
      "Iterations 709: training loss: 0.6756141098107855, testing loss: 0.7633977436999528\n",
      "Iterations 710: training loss: 0.6765972622469593, testing loss: 0.7826520738195806\n",
      "Iterations 711: training loss: 0.6761197454408276, testing loss: 0.7638036311212268\n",
      "Iterations 712: training loss: 0.6773248914166676, testing loss: 0.7840908357617216\n",
      "Iterations 713: training loss: 0.6768586131937555, testing loss: 0.7644628250976481\n",
      "Iterations 714: training loss: 0.6782687508528993, testing loss: 0.7858306958037413\n",
      "Iterations 715: training loss: 0.6778374978770371, testing loss: 0.7653600720362873\n",
      "Iterations 716: training loss: 0.6795627563629671, testing loss: 0.7880564242136072\n",
      "Iterations 717: training loss: 0.6791456810054983, testing loss: 0.7666042891475708\n",
      "Iterations 718: training loss: 0.6811583178309034, testing loss: 0.7906795619461391\n",
      "Iterations 719: training loss: 0.6807247025683137, testing loss: 0.7681388329918464\n",
      "Iterations 720: training loss: 0.6831142513097791, testing loss: 0.7937905876086261\n",
      "Iterations 721: training loss: 0.682655920046811, testing loss: 0.7700559089308612\n",
      "Iterations 722: training loss: 0.6854869144665281, testing loss: 0.7974409109750462\n",
      "Iterations 723: training loss: 0.6849876106097627, testing loss: 0.7724057841964577\n",
      "Iterations 724: training loss: 0.6881881088194371, testing loss: 0.8014810948681415\n",
      "Iterations 725: training loss: 0.6876336325985603, testing loss: 0.7751244068021953\n",
      "Iterations 726: training loss: 0.6911792604290438, testing loss: 0.8058638495802618\n",
      "Iterations 727: training loss: 0.6905372301605635, testing loss: 0.7781621238890921\n",
      "Iterations 728: training loss: 0.6944877316707536, testing loss: 0.8106051684369308\n",
      "Iterations 729: training loss: 0.693670861368394, testing loss: 0.7814795447210997\n",
      "Iterations 730: training loss: 0.6980161734023433, testing loss: 0.8156245950352353\n",
      "Iterations 731: training loss: 0.6969782440406737, testing loss: 0.7849884972319434\n",
      "Iterations 732: training loss: 0.7015075892343194, testing loss: 0.820562850236479\n",
      "Iterations 733: training loss: 0.7001479449470309, testing loss: 0.7883789107751257\n",
      "Iterations 734: training loss: 0.7047931199224011, testing loss: 0.8251944873409088\n",
      "Iterations 735: training loss: 0.7030953525403326, testing loss: 0.7915933959526271\n",
      "Iterations 736: training loss: 0.7076719404773116, testing loss: 0.8292551803292764\n",
      "Iterations 737: training loss: 0.7055095630467157, testing loss: 0.7942809037888193\n",
      "Iterations 738: training loss: 0.7099413950859783, testing loss: 0.8324694236834626\n",
      "Iterations 739: training loss: 0.7072736554546908, testing loss: 0.7962845667730984\n",
      "Iterations 740: training loss: 0.7110433013694522, testing loss: 0.8341980804909065\n",
      "Iterations 741: training loss: 0.7077002279617852, testing loss: 0.7968555827056654\n",
      "Iterations 742: training loss: 0.7107195092317409, testing loss: 0.8340663905630644\n",
      "Iterations 743: training loss: 0.7067383245387998, testing loss: 0.7959906583768049\n",
      "Iterations 744: training loss: 0.708966620355047, testing loss: 0.8320628240689674\n",
      "Iterations 745: training loss: 0.7043924123676938, testing loss: 0.7936523998729421\n",
      "Iterations 746: training loss: 0.7058701813966602, testing loss: 0.8283126594829745\n",
      "Iterations 747: training loss: 0.7008077303718482, testing loss: 0.7899734624986294\n",
      "Iterations 748: training loss: 0.7015717373662859, testing loss: 0.8229636845170861\n",
      "Iterations 749: training loss: 0.6962064158872098, testing loss: 0.7851910364476089\n",
      "Iterations 750: training loss: 0.6963230379833231, testing loss: 0.8163383812215137\n",
      "Iterations 751: training loss: 0.6908591609988698, testing loss: 0.7796308460080891\n",
      "Iterations 752: training loss: 0.6908688700716884, testing loss: 0.8093785460080775\n",
      "Iterations 753: training loss: 0.6855212874678595, testing loss: 0.7740757834393734\n",
      "Iterations 754: training loss: 0.6850593309385432, testing loss: 0.8018583606997105\n",
      "Iterations 755: training loss: 0.6799785930909328, testing loss: 0.7683206739065214\n",
      "Iterations 756: training loss: 0.6794115870249752, testing loss: 0.7944747580836946\n",
      "Iterations 757: training loss: 0.6747386076583776, testing loss: 0.7629230844106863\n",
      "Iterations 758: training loss: 0.6741355861593696, testing loss: 0.7874926963605797\n",
      "Iterations 759: training loss: 0.669999951014082, testing loss: 0.7581001665545622\n",
      "Iterations 760: training loss: 0.669257520435932, testing loss: 0.7809520182240699\n",
      "Iterations 761: training loss: 0.6655663655256838, testing loss: 0.7536354501902228\n",
      "Iterations 762: training loss: 0.6648078482983317, testing loss: 0.7749170984309993\n",
      "Iterations 763: training loss: 0.6616144654066207, testing loss: 0.749676217698022\n",
      "Iterations 764: training loss: 0.6609251756486139, testing loss: 0.7695810318304451\n",
      "Iterations 765: training loss: 0.6582029716919461, testing loss: 0.7462649888852686\n",
      "Iterations 766: training loss: 0.6575415729719312, testing loss: 0.7649118127541157\n",
      "Iterations 767: training loss: 0.6551954143129445, testing loss: 0.7432878337699218\n",
      "Iterations 768: training loss: 0.6546643050111061, testing loss: 0.760929597368078\n",
      "Iterations 769: training loss: 0.6526597746145841, testing loss: 0.7407778544525835\n",
      "Iterations 770: training loss: 0.652154383452588, testing loss: 0.7574545039212517\n",
      "Iterations 771: training loss: 0.6504179507987053, testing loss: 0.7385982861677644\n",
      "Iterations 772: training loss: 0.6499835386170811, testing loss: 0.7544398278247849\n",
      "Iterations 773: training loss: 0.6484673615098939, testing loss: 0.7367314542835159\n",
      "Iterations 774: training loss: 0.6480367713658323, testing loss: 0.7517399899801802\n",
      "Iterations 775: training loss: 0.6467403160661699, testing loss: 0.7351021848695888\n",
      "Iterations 776: training loss: 0.6464153362121251, testing loss: 0.7495373332312966\n",
      "Iterations 777: training loss: 0.6452691639499137, testing loss: 0.7337035198034125\n",
      "Iterations 778: training loss: 0.6449891920269938, testing loss: 0.7476398286666146\n",
      "Iterations 779: training loss: 0.6439498239390017, testing loss: 0.7324369652246935\n",
      "Iterations 780: training loss: 0.6436936810876205, testing loss: 0.7459493460862021\n",
      "Iterations 781: training loss: 0.6427564116730523, testing loss: 0.7312905140704309\n",
      "Iterations 782: training loss: 0.6425292064824797, testing loss: 0.744488550740844\n",
      "Iterations 783: training loss: 0.6416873579504495, testing loss: 0.7302447557105526\n",
      "Iterations 784: training loss: 0.6414931987621821, testing loss: 0.7432510578198686\n",
      "Iterations 785: training loss: 0.6407212458625042, testing loss: 0.7292788359779037\n",
      "Iterations 786: training loss: 0.6405853542420694, testing loss: 0.7422388591097356\n",
      "Iterations 787: training loss: 0.6398763586958166, testing loss: 0.7283982320098256\n",
      "Iterations 788: training loss: 0.6397915434681521, testing loss: 0.7414154744428791\n",
      "Iterations 789: training loss: 0.6391267243035151, testing loss: 0.7276115798390661\n",
      "Iterations 790: training loss: 0.6390812936476695, testing loss: 0.7407250451431006\n",
      "Iterations 791: training loss: 0.6384642743693038, testing loss: 0.7269107558631562\n",
      "Iterations 792: training loss: 0.6384672631365207, testing loss: 0.7401907949546999\n",
      "Iterations 793: training loss: 0.6379033703600862, testing loss: 0.7262860790009065\n",
      "Iterations 794: training loss: 0.6379680493823983, testing loss: 0.7398618592291801\n",
      "Iterations 795: training loss: 0.6374456152836623, testing loss: 0.7257410600520505\n",
      "Iterations 796: training loss: 0.6376032825075159, testing loss: 0.7397930111150545\n",
      "Iterations 797: training loss: 0.6371195768875559, testing loss: 0.7252804755685118\n",
      "Iterations 798: training loss: 0.6373980043154303, testing loss: 0.7399697719568572\n",
      "Iterations 799: training loss: 0.6369446746114454, testing loss: 0.7249707721204062\n",
      "Iterations 800: training loss: 0.6373681440735776, testing loss: 0.740421676525232\n",
      "Iterations 801: training loss: 0.6369608593187225, testing loss: 0.7248246218697412\n",
      "Iterations 802: training loss: 0.6375364679245111, testing loss: 0.7411962174748208\n",
      "Iterations 803: training loss: 0.6371746723536121, testing loss: 0.7248505416160701\n",
      "Iterations 804: training loss: 0.6379377794126188, testing loss: 0.7423378050605423\n",
      "Iterations 805: training loss: 0.637611568454053, testing loss: 0.7251002436517568\n",
      "Iterations 806: training loss: 0.6386258139787079, testing loss: 0.7438854628778728\n",
      "Iterations 807: training loss: 0.6383024667505554, testing loss: 0.7255999734728253\n",
      "Iterations 808: training loss: 0.6396531069907685, testing loss: 0.7459827628256384\n",
      "Iterations 809: training loss: 0.6393766603180803, testing loss: 0.7264743392787817\n",
      "Iterations 810: training loss: 0.641149233170656, testing loss: 0.7487353562860555\n",
      "Iterations 811: training loss: 0.6408749307015766, testing loss: 0.7277950875790992\n",
      "Iterations 812: training loss: 0.6432206481165627, testing loss: 0.7523278127178673\n",
      "Iterations 813: training loss: 0.6429610644030136, testing loss: 0.7297170464557564\n",
      "Iterations 814: training loss: 0.6459856274151455, testing loss: 0.7568662404152557\n",
      "Iterations 815: training loss: 0.6457770040291816, testing loss: 0.7324100426994588\n",
      "Iterations 816: training loss: 0.6497360277921254, testing loss: 0.7627348225668079\n",
      "Iterations 817: training loss: 0.6495565000382226, testing loss: 0.7361619273023914\n",
      "Iterations 818: training loss: 0.6544731319573711, testing loss: 0.7698958642132216\n",
      "Iterations 819: training loss: 0.6543190432473709, testing loss: 0.7410523861458895\n",
      "Iterations 820: training loss: 0.6606483351632619, testing loss: 0.7789387584917636\n",
      "Iterations 821: training loss: 0.6605243322597717, testing loss: 0.7475754262495881\n",
      "Iterations 822: training loss: 0.6685777754897315, testing loss: 0.7901882151947279\n",
      "Iterations 823: training loss: 0.6684866306300441, testing loss: 0.7560197487398622\n",
      "Iterations 824: training loss: 0.6785375760228742, testing loss: 0.8039819268374481\n",
      "Iterations 825: training loss: 0.6785796428859235, testing loss: 0.7668268072570041\n",
      "Iterations 826: training loss: 0.6901634023044841, testing loss: 0.8197155102159518\n",
      "Iterations 827: training loss: 0.6901484005864283, testing loss: 0.7793279565827371\n",
      "Iterations 828: training loss: 0.7036398716373554, testing loss: 0.8375523831129915\n",
      "Iterations 829: training loss: 0.7035850739539343, testing loss: 0.7940367467613436\n",
      "Iterations 830: training loss: 0.7178664878278643, testing loss: 0.85593645953001\n",
      "Iterations 831: training loss: 0.7175962078705737, testing loss: 0.8096156318914476\n",
      "Iterations 832: training loss: 0.7317134016856681, testing loss: 0.8735152927877713\n",
      "Iterations 833: training loss: 0.7307657187205039, testing loss: 0.824435304203134\n",
      "Iterations 834: training loss: 0.7424402026278769, testing loss: 0.88688887473914\n",
      "Iterations 835: training loss: 0.7400584619333872, testing loss: 0.8351708700616665\n",
      "Iterations 836: training loss: 0.7471341217282206, testing loss: 0.8925913667967145\n",
      "Iterations 837: training loss: 0.7420951445724644, testing loss: 0.8380737364205371\n",
      "Iterations 838: training loss: 0.7440353233408282, testing loss: 0.8887090209104741\n",
      "Iterations 839: training loss: 0.7354204271867227, testing loss: 0.8314662275560263\n",
      "Iterations 840: training loss: 0.7323733552432009, testing loss: 0.8742631622002061\n",
      "Iterations 841: training loss: 0.7201757938386756, testing loss: 0.8153739893389517\n",
      "Iterations 842: training loss: 0.7144854424920677, testing loss: 0.852002540664346\n",
      "Iterations 843: training loss: 0.7000547171559534, testing loss: 0.7938053604911113\n",
      "Iterations 844: training loss: 0.6935257170674828, testing loss: 0.8256195900258083\n",
      "Iterations 845: training loss: 0.6789203663855296, testing loss: 0.7710203131214345\n",
      "Iterations 846: training loss: 0.673136928948845, testing loss: 0.7993877521643443\n",
      "Iterations 847: training loss: 0.6604917881455792, testing loss: 0.7510853709373353\n",
      "Iterations 848: training loss: 0.655560150169632, testing loss: 0.7761286727993115\n",
      "Iterations 849: training loss: 0.6456425723123064, testing loss: 0.7351754111547072\n",
      "Iterations 850: training loss: 0.6417572572185891, testing loss: 0.7572898459916863\n",
      "Iterations 851: training loss: 0.6345236049350199, testing loss: 0.7235609329027675\n",
      "Iterations 852: training loss: 0.631493439153484, testing loss: 0.74269541334271\n",
      "Iterations 853: training loss: 0.6265537649514569, testing loss: 0.7155366769787439\n",
      "Iterations 854: training loss: 0.6242902511544699, testing loss: 0.7319815693155853\n",
      "Iterations 855: training loss: 0.6209776644653716, testing loss: 0.7101252515901276\n",
      "Iterations 856: training loss: 0.6192375799251012, testing loss: 0.7242097247533192\n",
      "Iterations 857: training loss: 0.6169620518594916, testing loss: 0.706430404589865\n",
      "Iterations 858: training loss: 0.6156679664937772, testing loss: 0.7185574920270731\n",
      "Iterations 859: training loss: 0.6140489924798875, testing loss: 0.7038779048187086\n",
      "Iterations 860: training loss: 0.6131138171753745, testing loss: 0.7144746298158047\n",
      "Iterations 861: training loss: 0.6119091996445505, testing loss: 0.7020880288365824\n",
      "Iterations 862: training loss: 0.6111211164063656, testing loss: 0.7112825167739006\n",
      "Iterations 863: training loss: 0.6101883845635716, testing loss: 0.700711385987518\n",
      "Iterations 864: training loss: 0.6095014014964478, testing loss: 0.7086829872644453\n",
      "Iterations 865: training loss: 0.6087522277056916, testing loss: 0.6996154991112997\n",
      "Iterations 866: training loss: 0.6081752216641452, testing loss: 0.7066307315185937\n",
      "Iterations 867: training loss: 0.6075279571238201, testing loss: 0.6986697845680649\n",
      "Iterations 868: training loss: 0.607008905675556, testing loss: 0.7049089602146962\n",
      "Iterations 869: training loss: 0.6064339703464299, testing loss: 0.6978194465176067\n",
      "Iterations 870: training loss: 0.6059478396890301, testing loss: 0.7033895933762474\n",
      "Iterations 871: training loss: 0.6054276477777579, testing loss: 0.697034953728328\n",
      "Iterations 872: training loss: 0.6049665926790884, testing loss: 0.7020370465005918\n",
      "Iterations 873: training loss: 0.6044790430880081, testing loss: 0.696285163300967\n",
      "Iterations 874: training loss: 0.6040315794820827, testing loss: 0.7008111909557525\n",
      "Iterations 875: training loss: 0.60356876827166, testing loss: 0.6955368890252753\n",
      "Iterations 876: training loss: 0.6031402733940597, testing loss: 0.699733410651903\n",
      "Iterations 877: training loss: 0.6026911416155002, testing loss: 0.6947789877092994\n",
      "Iterations 878: training loss: 0.6022720161877013, testing loss: 0.6987156719946329\n",
      "Iterations 879: training loss: 0.6018357301097118, testing loss: 0.6940078756096085\n",
      "Iterations 880: training loss: 0.6014238996459821, testing loss: 0.6977643102437869\n",
      "Iterations 881: training loss: 0.6009990034795388, testing loss: 0.6932602798129058\n",
      "Iterations 882: training loss: 0.6005953926713987, testing loss: 0.6968568451646131\n",
      "Iterations 883: training loss: 0.6001753191430648, testing loss: 0.6925242885574783\n",
      "Iterations 884: training loss: 0.5997815234998977, testing loss: 0.6959922490342048\n",
      "Iterations 885: training loss: 0.5993669961656906, testing loss: 0.6917805960294043\n",
      "Iterations 886: training loss: 0.598974942180398, testing loss: 0.6951478465983026\n",
      "Iterations 887: training loss: 0.5985607853574665, testing loss: 0.6910446462519448\n",
      "Iterations 888: training loss: 0.5981727026608273, testing loss: 0.6943330486692325\n",
      "Iterations 889: training loss: 0.5977623393881181, testing loss: 0.6903081390718478\n",
      "Iterations 890: training loss: 0.597376826840048, testing loss: 0.6935328470720867\n",
      "Iterations 891: training loss: 0.5969703889436748, testing loss: 0.6895807109839363\n",
      "Iterations 892: training loss: 0.596585621008847, testing loss: 0.6927554494019317\n",
      "Iterations 893: training loss: 0.5961831945043232, testing loss: 0.688852858014812\n",
      "Iterations 894: training loss: 0.5958014519927143, testing loss: 0.6920010337310818\n",
      "Iterations 895: training loss: 0.5954046180286918, testing loss: 0.6881113846319424\n",
      "Iterations 896: training loss: 0.59502579442097, testing loss: 0.691289119970763\n",
      "Iterations 897: training loss: 0.5946335107648377, testing loss: 0.6873688518768005\n",
      "Iterations 898: training loss: 0.5942581908509452, testing loss: 0.6906070740770538\n",
      "Iterations 899: training loss: 0.5938710567986761, testing loss: 0.6866217962545357\n",
      "Iterations 900: training loss: 0.5934996577342638, testing loss: 0.6899352098480237\n",
      "Iterations 901: training loss: 0.5931181774272393, testing loss: 0.6858764349350714\n",
      "Iterations 902: training loss: 0.592752573388579, testing loss: 0.6893181258295169\n",
      "Iterations 903: training loss: 0.592375559538338, testing loss: 0.6851148474482663\n",
      "Iterations 904: training loss: 0.5920156346969099, testing loss: 0.6887310606998378\n",
      "Iterations 905: training loss: 0.591645874142689, testing loss: 0.6843530442266991\n",
      "Iterations 906: training loss: 0.5912942820847544, testing loss: 0.6881887830526302\n",
      "Iterations 907: training loss: 0.5909361578422663, testing loss: 0.6835902900780667\n",
      "Iterations 908: training loss: 0.5905946193751913, testing loss: 0.6877066463386589\n",
      "Iterations 909: training loss: 0.590249594160556, testing loss: 0.6828256031374474\n",
      "Iterations 910: training loss: 0.5899245666675055, testing loss: 0.6873131730281625\n",
      "Iterations 911: training loss: 0.5895845651902523, testing loss: 0.6820733996170582\n",
      "Iterations 912: training loss: 0.5892798645034716, testing loss: 0.6869658437362058\n",
      "Iterations 913: training loss: 0.5889537505947557, testing loss: 0.6813234273389797\n",
      "Iterations 914: training loss: 0.5886781125996735, testing loss: 0.6867246168844202\n",
      "Iterations 915: training loss: 0.5883746976892361, testing loss: 0.6805961121329647\n",
      "Iterations 916: training loss: 0.5881460886113847, testing loss: 0.6866377748598746\n",
      "Iterations 917: training loss: 0.587876292017056, testing loss: 0.6798994134788385\n",
      "Iterations 918: training loss: 0.5877100479646533, testing loss: 0.6867430444484404\n",
      "Iterations 919: training loss: 0.5874584123810604, testing loss: 0.6792470609414707\n",
      "Iterations 920: training loss: 0.587374046743258, testing loss: 0.6870348113266309\n",
      "Iterations 921: training loss: 0.58717195355157, testing loss: 0.6786893717102619\n",
      "Iterations 922: training loss: 0.5872045160359098, testing loss: 0.6876243524464842\n",
      "Iterations 923: training loss: 0.5870855792163484, testing loss: 0.6782745562641089\n",
      "Iterations 924: training loss: 0.5872876091026398, testing loss: 0.6886784294802121\n",
      "Iterations 925: training loss: 0.5871982064240991, testing loss: 0.6780275466793686\n",
      "Iterations 926: training loss: 0.5876229080224324, testing loss: 0.6901546992236669\n",
      "Iterations 927: training loss: 0.5876106685441305, testing loss: 0.6780655410666633\n",
      "Iterations 928: training loss: 0.5883657172973421, testing loss: 0.692292872415089\n",
      "Iterations 929: training loss: 0.5885297835523731, testing loss: 0.6785487500795259\n",
      "Iterations 930: training loss: 0.5897791335474208, testing loss: 0.6955184855685674\n",
      "Iterations 931: training loss: 0.5900887539578685, testing loss: 0.6796570075960247\n",
      "Iterations 932: training loss: 0.5920339128345502, testing loss: 0.7000057364885722\n",
      "Iterations 933: training loss: 0.5925816466667877, testing loss: 0.6817340383765542\n",
      "Iterations 934: training loss: 0.595632883165856, testing loss: 0.706476994070675\n",
      "Iterations 935: training loss: 0.5964990440752365, testing loss: 0.6852544956434817\n",
      "Iterations 936: training loss: 0.601263303488479, testing loss: 0.7158617458890629\n",
      "Iterations 937: training loss: 0.6024691831122773, testing loss: 0.6909829687637797\n",
      "Iterations 938: training loss: 0.6098820171574169, testing loss: 0.7293962689484403\n",
      "Iterations 939: training loss: 0.6116358291573174, testing loss: 0.7001755918799882\n",
      "Iterations 940: training loss: 0.6231027517202157, testing loss: 0.7490360757204726\n",
      "Iterations 941: training loss: 0.6257478192238882, testing loss: 0.7149318274635915\n",
      "Iterations 942: training loss: 0.6436324696339164, testing loss: 0.7779978412129372\n",
      "Iterations 943: training loss: 0.6481729263130757, testing loss: 0.7389103307232292\n",
      "Iterations 944: training loss: 0.6761908142257715, testing loss: 0.8217164161153705\n",
      "Iterations 945: training loss: 0.6850484503342932, testing loss: 0.7788960891088487\n",
      "Iterations 946: training loss: 0.7274522290506111, testing loss: 0.8872873904943357\n",
      "Iterations 947: training loss: 0.7457876002857969, testing loss: 0.8450815254395061\n",
      "Iterations 948: training loss: 0.7979045039070555, testing loss: 0.973110496432106\n",
      "Iterations 949: training loss: 0.8319008154049417, testing loss: 0.9395506973893799\n",
      "Iterations 950: training loss: 0.8651006529298515, testing loss: 1.0511939196304698\n",
      "Iterations 951: training loss: 0.9107833009267987, testing loss: 1.0276617229080862\n",
      "Iterations 952: training loss: 0.8732421161486822, testing loss: 1.0581617055127395\n",
      "Iterations 953: training loss: 0.9024687043450172, testing loss: 1.022753802345116\n",
      "Iterations 954: training loss: 0.8230191078255057, testing loss: 0.9970101354045239\n",
      "Iterations 955: training loss: 0.8174407585196986, testing loss: 0.9341596672362622\n",
      "Iterations 956: training loss: 0.757466544338101, testing loss: 0.916958389322635\n",
      "Iterations 957: training loss: 0.7313429218970423, testing loss: 0.8412164974122192\n",
      "Iterations 958: training loss: 0.6935167946474434, testing loss: 0.8386810841673005\n",
      "Iterations 959: training loss: 0.6640269317987835, testing loss: 0.7659131440511362\n",
      "Iterations 960: training loss: 0.6430852082501981, testing loss: 0.7755454908112637\n",
      "Iterations 961: training loss: 0.6212994234976668, testing loss: 0.7173051513656407\n",
      "Iterations 962: training loss: 0.6096127877854974, testing loss: 0.7314601243828323\n",
      "Iterations 963: training loss: 0.597032613082085, testing loss: 0.6901315839250652\n",
      "Iterations 964: training loss: 0.5907872776888553, testing loss: 0.7048772007573209\n",
      "Iterations 965: training loss: 0.5844261396536498, testing loss: 0.6768572965715621\n",
      "Iterations 966: training loss: 0.5811895681297373, testing loss: 0.690157855908539\n",
      "Iterations 967: training loss: 0.5780160846825959, testing loss: 0.670660975913647\n",
      "Iterations 968: training loss: 0.5762285826004419, testing loss: 0.6819434900659619\n",
      "Iterations 969: training loss: 0.5744679658701775, testing loss: 0.6676208213765179\n",
      "Iterations 970: training loss: 0.573378338795067, testing loss: 0.6769434645111126\n",
      "Iterations 971: training loss: 0.5722742429910969, testing loss: 0.6660066187756513\n",
      "Iterations 972: training loss: 0.5714775456802236, testing loss: 0.6735432216766122\n",
      "Iterations 973: training loss: 0.5706983507740948, testing loss: 0.6649983147258586\n",
      "Iterations 974: training loss: 0.5700678329282892, testing loss: 0.6711054469773714\n",
      "Iterations 975: training loss: 0.569439954738341, testing loss: 0.6642266085989299\n",
      "Iterations 976: training loss: 0.5688994445334371, testing loss: 0.6691998982066877\n",
      "Iterations 977: training loss: 0.5683609072952958, testing loss: 0.6635656814838699\n",
      "Iterations 978: training loss: 0.5678786993025797, testing loss: 0.6676858447858093\n",
      "Iterations 979: training loss: 0.5673857873773656, testing loss: 0.6629476046564016\n",
      "Iterations 980: training loss: 0.566927801570349, testing loss: 0.6663622843416614\n",
      "Iterations 981: training loss: 0.5664676657971806, testing loss: 0.6623493188928897\n",
      "Iterations 982: training loss: 0.5660295633099606, testing loss: 0.6652053850437134\n",
      "Iterations 983: training loss: 0.565591855225396, testing loss: 0.6617566018925248\n",
      "Iterations 984: training loss: 0.5651703345669061, testing loss: 0.6641699462774331\n",
      "Iterations 985: training loss: 0.5647473132310515, testing loss: 0.6611619404045638\n",
      "Iterations 986: training loss: 0.5643383618023908, testing loss: 0.6632341243610133\n",
      "Iterations 987: training loss: 0.5639265261655171, testing loss: 0.6605487100943646\n",
      "Iterations 988: training loss: 0.5635217711144169, testing loss: 0.6623645743717478\n",
      "Iterations 989: training loss: 0.5631140384615922, testing loss: 0.6599830652411874\n",
      "Iterations 990: training loss: 0.5627127870862954, testing loss: 0.6615148823840746\n",
      "Iterations 991: training loss: 0.5623125680065597, testing loss: 0.6593632376244568\n",
      "Iterations 992: training loss: 0.5619145390562353, testing loss: 0.6607130955922459\n",
      "Iterations 993: training loss: 0.5615191091870364, testing loss: 0.6587368036138435\n",
      "Iterations 994: training loss: 0.5611255649214275, testing loss: 0.6599310107109098\n",
      "Iterations 995: training loss: 0.5607331289346656, testing loss: 0.658111019927572\n",
      "Iterations 996: training loss: 0.5603443439330711, testing loss: 0.6591863464846058\n",
      "Iterations 997: training loss: 0.5599545566882791, testing loss: 0.6574641576662479\n",
      "Iterations 998: training loss: 0.5595666692307363, testing loss: 0.6584563566393302\n",
      "Iterations 999: training loss: 0.5591796047167709, testing loss: 0.6568202418351424\n",
      "Iterations 1000: training loss: 0.5587952975636277, testing loss: 0.6577670489180313\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7cce0ff6de80>,\n",
       " <matplotlib.lines.Line2D at 0x7cce0ffebe00>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layer_sizes = [features_train.shape[1], 128, 4]  # structure\n",
    "params = init_params(layer_sizes)\n",
    "num_iters = 1000\n",
    "learning_rate = 3e-6  # 0.00001 \n",
    "losses_train, losses_test = [], []\n",
    "\n",
    "for i in range(num_iters):\n",
    "    preds_train, cache = forward(features_train, params)\n",
    "    preds_test, _ = forward(features_test, params)\n",
    "    loss_train = ce_loss(preds_train, labels_train)\n",
    "    loss_test = ce_loss(preds_test, labels_test)\n",
    "    print(f\"Iterations {i+1}: training loss: {loss_train}, testing loss: {loss_test}\")\n",
    "    losses_train.append(loss_train)\n",
    "    losses_test.append(loss_test)\n",
    "    grads = grad(preds_train, labels_train, params, cache)\n",
    "    for j in range(len(layer_sizes) - 1):\n",
    "        params['W' + str(j+1)] = params['W' + str(j+1)] - learning_rate * grads['dW' + str(j+1)]\n",
    "        params['b' + str(j+1)] = params['b' + str(j+1)] - learning_rate * grads['db' + str(j+1)]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(num_iters), losses_train, '--b', range(num_iters), losses_test, 'r')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Took more than 8 minutes to finish 1000 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 Accuracy Assess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7815126050420168\n"
     ]
    }
   ],
   "source": [
    "pred_classes_train = np.argmax(preds_train, axis=1)\n",
    "is_correct_train = pred_classes_train == raw_labels_train\n",
    "num_correct_train = sum(is_correct_train)\n",
    "accuracy_train = num_correct_train / len(raw_labels_train)\n",
    "print(accuracy_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3321",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
